<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22>
    <text y=%22.9em%22 font-size=%2290%22>üê∏</text></svg>">
    <meta name="description" content="Quality control and prediction" />
    <meta name="author" content="Adri√°n Ochoa Ferri√±o" />
    <title>Project 3: Waste understanding in Manufacturing</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://pyscript.net/releases/2025.3.1/core.css">
    <script type="module" src="https://pyscript.net/releases/2025.3.1/core.js"></script>
</head>

<body>
<py-config>
    packages = ["pandas", "scipy", "seaborn", "openpyxl"]
    [[fetch]]
    from = 'databases/'
    files = ['correlation_matrices.xlsx']
</py-config>

  <header>
    <nav>
      <ul>
        <li><a href="homepage_neu.html#project-showcase">Projekt-Schaufenster</a></li>
        <li><a class="translation" href="project_3.html">Deutsche Version</a></li>
      </ul>
    </nav>
  </header>


<section id="homepage">
  <h1>Understanding waste in Manufacturing:</h1>
  <h2>Quality control and quality prediction in an iron sponge plant. How interpretation of analyses and models can be
      used to control waste production and predict waste generation.
  </h2>
    <div class="project-container">
        <h3>SUSTAINABILITY has quickly become a core priority in modern manufacturing, driven by growing demand for
            products that are more eco-friendly and a lower impact on the environment. This is due to a new need within
            the manufacturing industry to be more environmentally conscious and uphold a higher responsibility on the
            impact that most of the manufacturing processes have.
            <br><br>
            In industrial production systems, particularly in energy- and resource-intensive sectors, waste manifest in
            various forms like material loss or energy consumption waste, which creates a necessity for companies to
            create digital solutions as a way to better understand waste in manufacturing processes by identifying and
            quantifying manageable elements that influence waste, enhance product quality and promotes a more
            profitable and sustainable operation.
            <br><br>
            The iron and steel industry faces these challenges constantly despite their advances in technology. The
            production of sponge iron (also known as direct reduced iron DRI) is famous for it's waste due to their
            nonlinear and highly complex thermochemical reactions that occur on a huge reactor under strict temperature
            and pressure conditions. As a result, monitoring and managing quality deviations and waste in real-time is
            a significant challenge. (AND also a key opportunity for data analysis and optimization modeling)
            <br><br>
            Thanks to the integration of advanced data analytics and machine learning techniques into manufacturing
            systems, it has become possible for thousands of sensors and sample data measurements to be processed for
            interpretation. In the context of sponge iron production, these tools allows to develop predictive models
            that link raw material quality, temperature and pressure sensor data within the direct reduction reactor,
            and the final product characteristics.
        </h3>

        <h3>This enables a dynamic quality control loop where deviations can be anticipated and corrected before they
            turn into waste at the end of the manufacturing process.
            <br><br>
            This project explores how data-driven interpretations of complex processes can support two main objectives:
            <b>Quality control</b> and <b>Quality prediction</b>. These objectives are approached through the lens of
            simulated process data typically encountered in a sponge iron plant.
            <br><br>
            Throughout this project we will use the <b>CRISP-DM</b> methodology in order to generate predictive models
            that can be used to forecast waste generation by identifying patterns through control sensors within a
            direct reduction reactor, and information obtained from material samples taken before and after the
            process, starting with the understanding of business objectives and ending with the evaluation of the
            model, along with actionable insights on the data.
            <br><br>
            Within this project, we will explore the concept of manufacturing sustainability and how interpretations of
            analyses and models can be used to control waste production and predict waste generation. By leveraging
            data-driven approaches which manufacturers can then use to identify the sources of waste, optimize their
            production processes, and reduce their environmental footprint.
            <br><br>
            The broader goal of this project is to showcase how manufacturers can leverage statistical modeling and
            machine learning for identifying sources of waste, monitor quality indicators and improve the utilization
            of resources.
            <br><br>
            <b>NOTE:</b> All data within this project is simulated and does not reflect real operational or sample data
            commonly found within actual existing sponge iron manufacturing plant. The conclusions reached at the end
            of the project does not truly represent findings that could be applicable to a real sponge iron production
            plant.
        </h3>

        <h3><b>STEP 1: Business Understanding</b><br><br>
            <b>I. Quality control:</b><br>
            Quality control is an essential process within manufacturing that aims to identify and correct any defects
            or deviations from the ideal production process. This project is developed for the analysis of data from
            various quality control sensors for the purpose of identifying patterns of quality issues considering all
            elements involved in the manufacturing process.
            <br><br>
            By pinpointing the sources and timing of these deviations, interventions can be applied in real time in
            order to stabilize product quality and reduce unnecessary material and energy waste.
            <br><br>
            <b>II. Quality prediction:</b><br>
            Quality prediction in manufacturing processes uses data and statistical modeling to forecast the quality of
            the final product based on the input variables and process parameters. The goal is to identify potential
            quality issues early in the production process, giving the manufacturer the opportunity of correcting the
            production before the generation of the final product.
            <br><br>
            In a sponge iron plant, this means forecasting final product characteristics (e.g., porosity,
            metallization, carbon content) from variables such as feed composition, reactor temperature, and pressure.
            Anticipating product quality enables proactive process adjustments that can prevent waste before it occurs.
            <br><br>
            For <b>Key Performance Indicators (KPI)</b> we'll take into consideration the value added to the
            manufacturing process when it's adapted to deal with waste production. The KPI's should promote a decrease
            in costs, an increase in profits, and a reduction of environmental impact. (As waste usually represent
            inefficient processes, low quality product or a combination of both)
        </h3>

        <h3><b>STEP 2: Data Understanding</b><br><br>
            Data understanding involves all activities related to exploring the structure, content, and quality of the
            datasets involved in the problem. For this manufacturing project, the primary focus is to assess the
            usability of the sensor readings, material samples and production data that is collected across different
            stages of the sponge iron DRI process simultaneously.
            <br><br>
            Before continuing the project, it's important to briefly introduce the underlying process. Sponge iron is a
            metallic product produced by the direct reduction of iron ore within a direct reduction reactor within a
            relatively controlled environment. Unlike blast furnaces, which uses coke (a type of solid fuel) as a
            reducing agent at lower temperatures. The reduction process involves using gases such as gases such as
            hydrogen (H<sub>2</sub>) and carbon monoxide (CO).
            <br><br>
            Within the DR reactor the iron ore experiences several chemical and thermodynamic processes such as "Redox"
            reaction, "Gas-solid" reactions with reducing agents, and Heat transfer mechanisms encompass a highly
            complex manufacturing process with dozens of material inputs and highly sensitive reactor conditions, and
            external agents introduced during processing. Which is why understanding how these variables interact and
            influence each other is critical before applying any quality control or predictive models.
            <br><br>
            Before applying any type of data models for quality control or quality prediction, we'll first have to
            understand the scale and range of the data. In this case, the project will take into account the production
            volume, raw material data, sensors within the reactor and quality control records.
            <br><br>
            <b>NOTE:</b> Within this project we won't consider any data quality issues like anomalies or missing data
            as all data is generated specifically for this project.
        </h3>

        <h3>The following are the main datasets included in this project:<br><br>
            <b>1. Raw material sample data:</b> these variables represent the physical and chemical characteristics,
            dimensions and composition of iron ore before it is introduced into the direct reduction reactor. Which is
            why understanding these input features is essential, as they influence the quality of reduction and the
            formation of impurities in the final product.<br>+ Ore size (cm)<br>+ Iron oxide purity % (Fe<sub>2</sub>
            O<sub>3</sub>)<br>+ Impurity 1 % (Al<sub>2</sub>O<sub>3</sub>)<br>+ Impurity 2 % (S)<br>+ Impurity 3 %
            (CaO)<br>+ Impurity 4 % (C)<br>+ Abrasion measure (mm¬≥)<br>+ K<sub>2</sub>Cr<sub>2</sub>O<sub>7</sub>
            content<br>+ MgO content<br>+ SiO<sub>2</sub> content<br>+ Unit of Porosity % (Œ¶)
        </h3>

        <img src="bilder/project_drei1.png" alt="Project dataset 1">

        <h3><b>2. Reactor sensors:</b> represent the internal environment of the direct reduction reactor which is
            monitored at different locations through the use of thermocouples and manometers in order to measure the
            temperature and pressure respectively. These sensors offer insights regarding the stability and consistency
            of the chemical reactions.
            <br>+ Temperature measurement 1 (C¬∞)<br>+ Pressure measurement 1 (Pa)<br>+ Temperature measurement 2 (C¬∞)
            <br>+ Pressure measurement 2 (Pa)<br>+ Temperature measurement 3 (C¬∞)<br>+ Pressure measurement 3 (Pa)
            <br>+ Temperature measurement 4 (C¬∞)<br>+ Pressure measurement 4 (Pa)<br>+ Temperature measurement 5 (C¬∞)
            <br>+ Pressure measurement 5 (Pa)
            <br><br>
            The code resamples this data into 30-minute intervals and computes rolling averages in order to smooth the
            small fluctuations that naturally occur within the reactor as a way to simulate real industrial monitoring
            systems.
        </h3>

        <img src="bilder/project_drei2.png" alt="DR Plant">

        <h3><b>3. Agent flows:</b> includes the injection of reagents, chemical binders and fluxes added within the
            direct reduction reactor for the purpose of chemically altering the iron pellets into sponge iron. Measured
            in flow rates. <br>+ Reagent 1 H<sub>2</sub> (L/min)<br>+ Reagent 2 CO (L/min)<br>+ Reagent 3 CH
            <sub>4</sub> (L/min)<br>+ Binder 1 Al<sub>2</sub>O<sub>3</sub> (kg/min)<br>+ Binder 2 4SiO<sub>2</sub>
            (kg/min)<br>+ Binder 3 C<sub>6</sub>H<sub>10</sub>O<sub>5</sub> (kg/min)<br>+ Binder 4 C<sub>12</sub>H
            <sub>22</sub>O<sub>11</sub> (kg/min)<br>+ Flux 1 CaCO<sub>3</sub> (L/min)<br>+ Flux 2 CaMgCO<sub>3</sub>
            <br>+ Flux 3 SiO<sub>2</sub>
        </h3>

        <h3>Agent flows are a very important element to the direct reduction process since every type of chemical
            component has a function to the overall state of the final product.<br><br>For example, the <b>Reagents</b>
            (or reducing gas) react with the iron oxide (FeO) in the direct reduction process to produce sponge iron
            (Fe), while <b>Binders</b> are powders that are then added to hold fibers together and create a cohesive
            structure of the iron pellets, and finally <b>Fluxes</b> serve to promote the chemical reactions that
            happen under certain environmental conditions.
            <br><br>
            Understanding the function of these compounds is key to linking their flow behavior to process outcomes.
            For example, variations in H‚ÇÇ flow may affect metallization, while inconsistencies in binder composition
            could increase abrasion or waste powder generation.
        </h3>

        <h3><b>4. DRI sample data:</b> serve as a way to analyze the distribution between the final product and the
            waste generated after the Direct Reduction process, as well as the quality of the DRI final product which is
            measured by the types of impurities commonly found within the iron ore.<br>+ DRI pellet %<br>+ Pellet size
            (cm)<br>+ DRI purity % (Fe)<br>+ Impurity 1 % (S)<br>+ Impurity 2 % (C)<br>+ Metallization %<br>+ Slag %
            <br>+ Waste powder %<br>+ Unit of Porosity % (Œ¶)
        </h3>

        <img src="bilder/project_drei3.png" alt="DRI Process">

        <h3><b>STEP 3: Data Preparation</b><br><br>
            Now that all data has been collected and preprocessed, we can observe that each dataset originates from a
            different sampling point within the direct reduction process. This reflects how actual slow industrial
            processes function, where data is generated asynchronously from multiple stages of production. For this
            reason, the model has to adapt the information by linking the output data with the multiple different
            sources of input data.
            <br><br>
            This allows the model to trace the transformation for every single pellet batch of iron ore throughout the
            entire DRI production cycle, by aligning the datasets at their relevant time-steps. In a typical DRI
            process, a complete transformation - from raw pellet input to reduced iron output - takes approximately 9
            hours.
        </h3>

        <section class="sample_code">
            <b># Python CODE: Aggregation of data</b>
            <br><b>In the code, this logic is applied directly to the df_raw_material dataframe by subtracting 9 hours
            from the timestamp. This aligns the raw material features with the corresponding DRI output for each
            batch.</b>
            <br><br>try:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df_raw_material['Date'] = pd.to_datetime(df_raw_material['Date'], format='%d/%m/%Y %H:%M')
            <br>except ValueError as e:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df_raw_material['Date'] = pd.to_datetime(df_raw_material['Date'], format='mixed')
            <br>df_raw_material['Date'] = df_raw_material['Date'] - pd.Timedelta(hours=9)
            <br><br><b>Next, we consider the sensor data collected from within the direct reduction (DR) reactor.
            These variables include pressure and temperature readings from various reactor zones, recorded at high
            frequency (every 2 minutes). To reduce granularity and enhance interpretability, these measurements are
            resampled to 30 minute intervals, calculating the mean and standard deviation as a way to capture the
            central tendencies but also it's operational variability.</b>
        </section>

        <h3>The final quality metrics observed in the DRI samples at time t are influenced by the raw material and
            operating conditions that occured at time t - 9 hours. Consequently, each of the four datasets introduced
            in the data understanding phase must undergo a temporal adjustment to reflect their role in this 9 hour
            transformation window.
            <br><br>
            For example, if a DRI pellet is sampled at around 4:00 PM, the raw material properties responsible for its
            formation would have been recorded at around 7:00 AM. In order to link both of these datasets, the data
            preparation phase has to shift the timestamp of the raw material dataset back by 9 hours. This adjustment
            allows us to construct a causal link between the input material data and the output DRI pellet quality
            data.
        </h3>

        <section class="sample_code">
            <b># Python CODE: Rolling window CODE</b>
            <br><br>try:
            resampled_df = df_reactor_sensor.resample('30T').agg({<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 1':
            ['mean', 'std'], 'Pressure 1': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 2': ['mean',
            'std'], 'Pressure 2': ['mean', 'std'], <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 3': ['mean', 'std'],
            'Pressure 3': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 4': ['mean', 'std'], 'Pressure
            4': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 5': ['mean', 'std'], 'Pressure 5':
            ['mean', 'std']})
            <br>resampled_df.reset_index(inplace=True)
            <b>However, aligning the sensor data with the DRI output is more complex. Because the reactor
            continuously processes material over the 9-hour period, the relevant sensor readings are those observed
            throughout the entire time window leading up to the sampling of the DRI. </b>
            <br><br><b>To account for this, a rolling window technique is applied across the sensor time series. The
            rolling average and rolling standard deviation over a 9-hour window are computed to represent the
            cumulative effect of reactor conditions on each pellet batch.</b>
            <br><br>df_reactor_sensor = df_reactor_sensor.rolling("9H").mean()
            <br>df_reactor_sensor = df_reactor_sensor.iloc[18:]
        </section>

        <h3>This technique is extended to the agent flow dataset as well. Similar to the sensors, the composition and
            quantity of reducing gases, binders and fluxes vary over time and influence the chemical reactions inside
            the reactor. Applying a rolling window to this dataset allows us to compute the average behaviours and
            fluctuations of key components over the same 9-hour period.
            <br><br>
            By using time-shifting and rolling window aggregation, we transform the original asynchronous datasets
            into a unified, temporally aligned dataset. Each row in the final version of the merged DataFrame now
            represents the same iron ore throughout the entire process linking: the raw material characteristics, the
            reactor's internal state during the pellet's transformation, the chemical agents involved, and the final
            DRI quality outcomes.
            <br><br>
            Thanks to aggregating the reactor data, we ensure that the input features and target variables reflect true
            process dependencies, thereby improving the relevance and accuracy of our quality prediction and waste
            estimation models. This also prevents any alignment of mismatched observations.
        </h3>

        <h3><b>STEP 4: Data Exploration and Visualization</b><br><br>
            With all datasets temporally aligned and merged into a unified structure, we can now proceed with the data
            exploration and visualization step that involves a number of statistical and visualization techniques that
            serve in understanding the statistical characteristics of the all of the aforementioned variables and
            uncovering relationships, identifying trends and anomalies, as well as preparing for informed variable
            selection in the modeling phase.
            <br><br>
            Given the high number of independent variables that are within the newly structured dataset (52 independent
            variables) it is logical to say that the model could face issues related to high dimensionality. High
            dimensionality datasets pose several challenges: they can lead to overfitting, increase of computacional
            strains, and unnecessary model complexity which can hamper the interpretability and robustness of
            predictive models like the one we are planning to build.
            <br><br>
            Dimensionality problems often arise when multiple features are strongly correlated, meaning they convey
            overlapping or redundant information. This redundancy can obscure the interpretability of statistical tests
            for the dependent DRI product variables, and can be remedied using <b>Dimensionality reduction</b>
            techniques. For which a correlation analysis has to be made.
        </h3>

        <section class="sample_code">
            <b>Pearson's correlation matrix</b> measures the linear relationship between two continuous variables,
            which in the case of our dataset, it is applicable to all prepared data. Pearson's correlation is commonly
            used when analyzing relationships between variables with continuous data, assuming that the data is
            normally distributed and that the relationship between all variables is linear.
            <br><br>
            <div id="pearson_heatmap">
                <py-script>
                import pandas as pd
                import seaborn as sns
                import openpyxl
                import matplotlib.pyplot as plt
                from pyscript import display

                df_1 = pd.read_excel('correlation_matrices.xlsx', sheet_name="Pearson Correlation", header=0,
                    index_col=0)

                fig1, ax1 = plt.subplots()
                sns.heatmap(df_1, annot=False, cmap="Blues", ax=ax1, xticklabels=False, yticklabels=False)
                plt.title("Pearson correlation (DRI process)")
                display(fig1, target="pearson_heatmap")
                </py-script>
            </div>

            <br><br>
            <b>Spearman's correlation matrix</b> measures instead the strength and direction of a
            monotonic relationship between two variables. Pearson's correlation coefficient does not assume linearity
            between the variables, nor does it assume that the data is normally distributed, which is particularly
            helpful if there are exponential, logarithmic or even polynomial relationships between two variables.
            <br><br>
            <div id="spearman_heatmap">
                <py-script>
                import pandas as pd
                import seaborn as sns
                import openpyxl
                import matplotlib.pyplot as plt
                from pyscript import display

                df_2 = pd.read_excel('correlation_matrices.xlsx', sheet_name="Spearman Correlation", header=0,
                    index_col=0)

                fig2, ax2 = plt.subplots()
                sns.heatmap(df_2, annot=False, cmap="Greens", ax=ax2, xticklabels=False, yticklabels=False)
                plt.title("Spearman correlation (DRI process)")
                display(fig2, target="spearman_heatmap")
                </py-script>
            </div>
        </section>

        <h3>Dimensionality reduction techniques are self-explanatory but they are a bit more advanced as simply
            reducing the number of variables, as it could inadvertently lose underlying informational structure of the
            data. In this project, two complementary dimensionality reduction techniques are applied and compared:
            Principal Component Analysis (PCA) and Reverse Stepwise Regression.
            <br><br>
            <b>Reverse Stepwise Regression</b> and <b>Principal Component Analysis (PCA)</b> both require an in-depth
            analysis of the relationships between independent variables as a way to determine which variables offer
            redundancy within the dataset. In this project, we will not only apply basic Pearson correlation metrics
            but also prove if there are non-linear relationships between the variables using Spearman's rank
            correlation coefficient.
            <br><br>
            PCA is a statistical technique that transforms the original features into a new set of uncorrelated
            components ranked by their ability to explain variance in the data. It is particularly useful when trying
            to preserve information while reducing the number of input dimensions for modeling. The cumulative
            explained variance is visualized to determine the optimal number of components needed to retain most of the
            information, and these components are then used to train a Generalized Additive Model (GAM).
            <br><br>
            Reverse Stepwise Regression, on the other hand, is a feature selection method that iteratively removes
            variables from a multivariate regression model based on statistical significance (p-values). This approach
            retains only those predictors that contribute meaningfully to the target variable, offering a more
            interpretable model in terms of actual variable names rather than abstract components.
            <br><br>
            <b>Note:</b> Reverse Stepwise Regression and PCA will help in understanding which variables impact the most
            in waste generation by isolating variables most responsible for quality variation and waste formation
            <b>(Quality Control)</b> and enable the creation of more accurate models that forecast waste levels and
            final product quality characteristics <b>(Quality prediction)</b>.
        </h3>

        <h3>The project is handling two different Dimensionality reduction techniques as a way to mitigate the flaws of
            them both. For "Reverse Stepwise Regression", the goal is to remove the least significant predictor
            (based on p-value) at each step, by fitting the complete dataset into a model to the data, evaluate its
            performance and then slowly remove all predictors that have the least significance until the performance of
            the model reaches it's peak.
            <br><br>
            On the other hand, Principal Component Analysis (PCA) is built by transforming the dataset into principal
            components, through the use of <b>Eigendecomposition</b> which decomposes the group of dependent
            variables into their respective eigenvectors and eigenvalues. PCA then selects the top Principal components
            by picking out eigenvectors based on their corresponding eigenvalues.
            <br><br>
            Unfortunately PCA comes at the cost of <b>interpretability</b> as these components are linear combinations
            of the original features. Reverse Stepwise Regression, on the other hand, retains actual feature names and
            provides transparency about which variables are most significant to waste generation and product quality.
            This means that both of the results obtained from the PCA and Reverse Stepwise Regression results could be
            used for training the forecasting model.
        </h3>

        <h3><b>STEP 5: Feature Engineering</b><br><br>
            For both cases, we will have to preprocess the dataset by scaling all data equally. This is done because
            most of the independent variables don't share the same metrics (Pressures, Temperatures, Percentages,
            Measurements, etc.). There are many different scaling methods from the sklearn.preprocessing library like
            StandardScaler, MinMaxScaler, RobustScaler, etc. that have the same goal of standardizing data.
            <br><br>
            Standardization ensures that variables contribute equally to model training and component generation. In
            this project, the MinMaxScaler is selected adter applying both "Shapiro-Wilk" and "Kolmogorov-Smirnov"
            tests for normativity and confirming that most variables DO NOT follow a normal distribution. Which is
            particularly well-suited for preparing industrial process data for downstream modeling, as MinMaxScaler
            utilizes linear scaling to a fixed range (-1 to 1) preserving the original shape of any of the non-normal
            distributions.
            <br><br>
            Once scaled, the data is passed through a PCA transformer object in order to identify the minimum number of
            components required to retain most of the dataset's variance while maintaining the minimum amount of
            variables for regression.
        </h3>

        <section class="sample_code">
            <b># Python CODE: MinMaxScaling<br>
            After applying both "Shapiro-Wilk" and "Kolmogorov-Smirnov" tests for normality and goodness of fit. We can
            determine that none of the independent variables follow a normal distribution which pushes us to use the
            MinMaxScaler as it's particularly effective when the data is not normally distributed, by applying a
            scaling method that doesn't assume any distribution and scales the data to a range of [-1, 1], preserving
            the original non-normal distribution.<br><br>
            Note: MinMaxScaler applies a uniform scaling by subtracting the minimum value and dividing by the range of
            the dataset.</b><br>
            <br>from sklearn.preprocessing import MinMaxScaler
            <br>scaler = MinMaxScaler()
            <br>independent_variables = final_variables.iloc[:, 1:].copy()
            <br>independent_variables.columns = independent_variables.columns.astype(str)
            <br>independent_variables = scaler.fit_transform(independent_variables)<br><br>
            <b># Python CODE: Principal Component Analysis (PCA)<br>
            In scikit-learn, PCA is implemented as a transformer object that learns components in its fit() function,
            which computes the covariance matrix, handles the eigendecomposition on the covariance matrix and then
            generates the principal components.</b><br>
            <br>from sklearn.decomposition import PCA
            <br>pca_dri = PCA()
            <br>pca_dri.fit(independent_variables)
            <br>principal_components = pca_dri.transform(independent_variables)
            <br>selected_components = principal_components[:, :6]
            <br><br><b>The PCA components will be later fed into a General Additive Model (GAM) regression model that
            will consider the complexity of the relationships between the independent and dependent variables, as well
            as offer an in-depth statistical analysis of the entire process.</b>
        </section>

        <h3>Thanks to the explained_variance_ratio_ function within sklearn.decomposition we can measure the
            percentages of variance explained by each of generated principal components. By calculating the accumulated
            sum of the ratios we can determine that the total variance of the first 6 PCA components reach a variance
            of 92.39%. (If we handle 5 Components the percentage is 87.97% and if we handle 7 PCA components the
            variance reaches 93.61%)
            <br><br>
            <b>Note:</b> If we use 14 or more components, the explained_variance_ratio_ is 1.0 indicating that we‚Äôve
            captured 100% of the variance in the dataset.
            <br><br>
            The feature selection from Reverse Stepwise Regression can serve as a useful check against the results from
            PCA. While PCA identifies patterns based on variance alone, stepwise regression selects features based on
            their predictive power. When the same variables appear as important in both methods, it adds confidence
            that those features are genuinely influential in the process and not just statistical noise.
            <br><br>
            Using both PCA and Reverse Stepwise Regression together allows us to reduce dimensionality while keeping
            the model interpretable. PCA helps simplify the dataset and manage multicollinearity, while stepwise
            regression highlights the specific variables most responsible for predicting outcomes. This combined
            approach strengthens the reliability of the model and ensures it reflects the underlying production
            process.
        </h3>

        <h3>Without scaling, PCA would provide misleading principal components and reduce the interpretability and
            performance of your GAM model that follows. Additionally, when variables have vastly different ranges,
            small errors in larger-valued variables can disproportionately affect the outcome. Scaling mitigates this
            by putting all variables on a comparable footing, which is particularly important in multicollinear
            systems‚Äîlike yours‚Äîwhere slight imbalances in scale can mask or exaggerate relationships.
        </h3>

        <video id="video_1" controls autoplay loop muted>
            <source src="videos/project_drei.mp4" type="video/mp4">
        </video>

        <h3><b>STEP 6: Model Building and Training</b><br><br>
            Like we mentioned before, the Generalized Additive Model (GAM) is used to estimate the relationship between
            the output variables  and the independent variables which is the raw materials and the DRI process. GAM
            serves as an extension of linear models that incorporates non-linear effects by applying smoothing
            functions to each of the PCA predictors.
            <br><br>
            Each PCA component is introduced within GAM and then transformed through the smoothing function with the
            use of ‚Äúsplines‚Äù. Splines are smooth, flexible mathematical functions used to model non-linear
            relationships between variables.
            <br><br>
            Since the PCA components contain the eigenvalues which measure the relevance to the output variables by
            measuring the variance each component captures. Keeping only components that are uncorrelated to each other
            (orthogonal) with large eigenvalues (carry more information).
        </h3>


        <h3>Once the principal components have been transformed through the splice functions, the GAM structure takes
            the following form:<br>
            <code>s(0) + s(1) + ... + s(5)</code>, where <code>s(i)</code> is a spline applied to the i-th principal
                component.
            <br><br>
            The 9 dependent (output) variables use the same set of PCA-derived features as a way to improve the
            interpretability of the data by analyzing the forecasting values of the model and allowing the targeted
            evaluation of how each output variable is influenced by the latent structure of the raw material and
            process data.
            <br><br>
            To assess the performance of each model, scatter plots are used to compare the predicted values against the
            actual observed values.
        </h3>

        <img src="bilder/project_drei4.png" alt="Project Scatter Graph 1">

        <h3>These plots include a smooth prediction curve overlaid on the scatter of true values, helping visualize
            both model fit and residual variance. The first principal component is particularly useful for
            visualization since it typically captures the largest source of variance in the input data, making it a
            strong proxy for dominant process behavior.
            <br><br>
            After building the GAM model using PCA-transformed inputs, the next step is to repeat the modeling process
            using Reverse Stepwise Regression. This technique helps identify the most relevant original features rather
            than transformed ones, which links the insights of the model directly to the physical variables from the
            raw material or DRI process data.
        </h3>

        <h3>Using the statsmodels library, reverse stepwise regression begins with all variables included in the model.
            The variable with the highest p-value (indicating the least statistical significance) is removed, and the
            model is refit. This iterative elimination continues until only the 6 most statistically significant
            variables remain. These selected features are then used as inputs to a second GAM model, following the same
            spline-based structure used for the PCA-derived model.
            <br><br>
            By comparing the performance and interpretability of both models‚Äîone based on PCA components, the other on
            statistically-selected raw features‚Äîwe can evaluate trade-offs between dimensionality reduction and
            variable transparency. The PCA-based model may offer smoother performance and reduced noise, while the
            stepwise model provides clearer links to specific process parameters, which is valuable for engineering
            insights and decision-making.
        </h3>

        <img src="bilder/project_drei5.png" alt="Project Scatter Graph 2">

        <h3>This logic is better represented within the two graphs that serve as the visualization of model diagnostics
            from the Generalized Additive Model (GAM) predictions. Both graphs represent the relationship between two
            different target variables with the same principal component. (Which is the first principal component
            within GAM)
            <br><br>
            The first graph displays the relationship between generated Slag (type of waste from the DR process) and
            the first principal component. The black line represents the forecasting values of the model, which has a
            strong negative correlation, as the first principal component increases, slag tends to decrease.
            <br><br>
            The second graph on the other hand displays the relationship between the DRI pellet percentage (which
            represents the overall quality of the final pellet product) and the first principal component. But in
            contrast to the first graph, the second graph shows how there‚Äôs no strong relationship between both
            elements.
        </h3>

        <h3><b>STEP 7: Model Evaluation and Comparison</b><br><br>
            To evaluate the predictive quality and generalizability of both modeling approaches‚ÄîPCA-based GAM and
            Reverse Stepwise Regression-based GAM‚Äîa set of diagnostic metrics was used: Generalized Cross-Validation
            (GCV), Akaike Information Criterion (AIC), and Pseudo R-squared. These metrics allows the assessment of how
            well each model fits the data but also how well it is expected to perform on unseen inputs.
            <br><br>
            The GCV value is a method used to estimate the prediction error of a model, particularly in situations
            involving regularization techniques like ridge regression and is a penalized error metric that accounts for
            model complexity between the two different models. GCV provides a way to assess a model's predictive
            performance and optimize its parameters.
        </h3>

        <h3>The PCA-based GAM achieved a GCV of 0.0001, suggesting minimal prediction error and a smooth generalization
            to new data. On the other hand, the Reverse Stepwise Regression model returned a GCV of 0.0, which, while
            appearing optimal, may in fact reflect overfitting a scenario where the model fits the training data too
            perfectly and may not generalize well.
            <br><br>
            This means that in order for the model to work with new data, it needs new training-test splits or
            cross-validations to ensure that the model generalizes well and is not overly fitted to specific patterns
            in the training set.
            <br><br>
            The Akaike Information Criterion (AIC) is a statistical measure used to evaluate the relative quality of
            the two different GAMs. It balances the importance between model fit (likelihood) and  model complexity
            (number of parameters) in order to avoid overfitting and poor performance on new data. Where lower AIC
            values generally indicate a better-fitting and more parsimonious model.
        </h3>

        <h3>Similar to the GCV value, the AIC penalizes models with more parameters, discouraging overly complex models
            that might overfit the data. Measuring the AIC in both models shows that the PCA model has a significantly
            lower AIC. This supports the idea that the PCA-based model achieves strong predictive capabilities while
            avoiding unnecessary complexity, despite the abstract creation of principal components.
            <br><br>
            In this project, one of the key metrics used to assess model performance is R-squared (R¬≤), also known as
            the coefficient of determination. R¬≤ measures how well a model explains the variability of the target
            variable based on the inputs. For instance, an R¬≤ of 0.82 indicates that 82% of the variation in a DRI
            quality metric (like slag content or pellet size) is accounted for by the model‚Äôs predictors.
            <br><br>
            When applied to Generalized Additive Models (GAMs), we refer to this metric as Pseudo R-squared because
            GAMs use smoothing splines and penalized likelihood estimation, which differ from traditional linear
            regression. In this context, Pseudo R¬≤ is calculated as the proportion of deviance reduced by the model,
            offering a similar interpretation: the higher the value, the better the model fits the data.
        </h3>

        <h3>The PCA-based GAM model achieved a Pseudo R¬≤ of 0.3382, indicating that about 33.82% of the variability in
            the outcome could be explained using just the top six principal components. While this may seem moderate,
            it reflects a model that balances generalization and robustness. In contrast, the Reverse Stepwise
            Regression model reached a much higher Pseudo R¬≤ of 0.8252, showing strong explanatory power but also
            suggesting a greater risk of overfitting‚Äîespecially when paired with an unusually low GCV value.
            <br><br>
            This deviation raises an important trade-off. While the reverse stepwise model has stronger explanatory
            power, it may do so at the expense of generalizability, as suggested by the GCV. The PCA model appears to
            achieve a better balance between predictive performance and model stability by reducing noise and
            redundancy through dimension reduction.
        </h3>

        <h3><b>STEP 8: Model Refinement</b><br><br>
            By evaluating both GAMs. One used PCA components. The other used statistically selected features from
            Reverse Stepwise Regression. It became clear that both approaches serve as dimensionality reduction tools.
            However, they differ significantly in terms of information retention, model complexity, and
            interpretability.
            <br><br>
            A major strength of the PCA-based model is its ability to preserve nearly all of the original variance
            (e.g., 92.39% is captured by six components) without overfitting. This enables the generalized additive
            model (GAM) to generalize well, reducing noise and multicollinearity while maintaining predictive strength.
            This is particularly valuable when dealing with high-dimensional data, as is common in industrial sensor
            networks and chemical process monitoring.
        </h3>

        <h3>However, PCA transforms the variables into latent components, which can make direct interpretation
            challenging. Unlike raw features, such as reagent flow or pressure levels, principal components do not
            correspond to single physical quantities. This lack of interpretability can limit its usefulness in process
            diagnostics and corrective actions, where domain experts often need to know the exact variable to adjust.
            <br><br>
            In addition to comparing methods, model refinement should consider scalability and deployment potential. In
            real-time manufacturing systems, models must be efficient and robust enough to withstand sensor noise and
            adapt to process drift. In this context, principal component analysis (PCA)-based models offer a favorable
            balance by simplifying the feature space, improving runtime performance, and offering resilience to
            fluctuations in individual sensor readings.
            <br><br>
            Several refinement strategies can be introduced to further improve model performance, reliability, and
            usability in real-world manufacturing settings. For instance, the current model uses default lambda values.
            Tuning lambda for each variable, either manually or through automated cross-validation, could improve the
            final model. This process adjusts the flexibility of each spline to find the optimal balance between
            capturing important trends and avoiding noise.
        </h3>

        <h3>Another important area for future improvement is the integration of residual analysis techniques into the
            model evaluation and refinement workflow. By systematically analyzing residual patterns, it becomes
            possible to identify whether certain relationships have been missed, whether non-linear effects are
            insufficiently captured, or whether the model consistently over- or under-predicts under specific process
            conditions.
            <br><br>
            Incorporating these techniques not only improves predictive accuracy but also strengthens the model‚Äôs
            reliability and transparency, making it easier to deploy in real-world production environments. In
            industrial applications, where decisions based on model outputs can affect cost, safety, and product
            quality, consistency and interpretability are just as critical as performance.
            <br><br>
            For this reason, model refinement is an essential step in any machine learning or statistical modeling
            project. It provides a structured way to enhance results through technical adjustments‚Äîsuch as
            hyperparameter tuning, diagnostic analysis, or regularization‚Äîwithout discarding the overall structure or
            objective of the model. These refinements can often lead to substantial improvements in performance and
            usability, making the model more resilient, efficient, and aligned with operational needs. In the context
            of manufacturing, this translates directly into better process control, reduced waste, and more consistent
            product quality.
        </h3>

        <h3><b>CONCLUSION:</b><br>
            This project demonstrates the potential of combining <b>Generalized Additive Models (GAM)</b> with
            dimensionality reduction techniques such as <b>Principal Component Analysis</b> and
            <b>Reverse Stepwise Regression</b> for improving quality control and waste prediction in the context of
            <b>Direct Reduced Iron (DRI)</b> production.
            <br><br>
            The GAM framework excels in its ability to capture non-linear relationships between inputs and outputs
            through smooth spline functions. This is particularly important in industrial environments, where many
            variables interact in non-obvious ways. PCA supports this modeling approach by transforming a
            high-dimensional feature space into a compact and noise-resistant set of components, helping improve
            generalization and reduce computational complexity. Meanwhile, Reverse Stepwise Regression ensures that
            interpretability is preserved by identifying the most statistically significant raw process
            variables‚Äîallowing for direct links between model behavior and engineering knowledge.
        </h3>

        <h3>These models not only enable accurate forecasting of key quality indicators (e.g., slag content, porosity,
            metallization) but also support root cause analysis by revealing which process conditions most strongly
            influence final product quality. The examination of <b>smooth spline functions</b> and
            <b>coefficient summaries</b> allows engineers to pinpoint influential parameters and understand how they
            interact‚Äîproviding actionable insights for improving both efficiency and sustainability.
            <br><br>
            Furthermore, this project establishes a blueprint for data-informed decision-making in manufacturing
            environments. It demonstrates how predictive modeling can go beyond monitoring to support early warnings,
            optimize resource usage, and reduce waste generation. The interpretability and flexibility of GAMs make
            them particularly suitable for implementation in real-time control systems, where both transparency and
            adaptability are required.
            <br><br>
            In conclusion, the integration of data-driven models like GAM into industrial systems can transform
            traditional process monitoring into a proactive and predictive framework, reducing waste, improving output
            quality, and supporting data-informed decision-making at scale. This framework is especially powerful in
            data-rich environments, where complex process dynamics can no longer be fully managed through manual
            control and static rules alone.
        </h3>
    </div>
</section>

  <footer>
    <p>Erstellt von Adri√°n Ochoa Ferri√±o - 2023</p>
  </footer>

</body>
</html>