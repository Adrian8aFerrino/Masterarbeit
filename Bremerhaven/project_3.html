<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22>
    <text y=%22.9em%22 font-size=%2290%22>üê∏</text></svg>">
    <meta name="description" content="Quality control and prediction" />
    <meta name="author" content="Adri√°n Ochoa Ferri√±o" />
    <title>Projekt 3: Verschwendungsverst√§ndnis in der Produktion</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://pyscript.net/releases/2025.3.1/core.css">
    <script type="module" src="https://pyscript.net/releases/2025.3.1/core.js"></script>
</head>

<body>
<py-config>
    packages = ["pandas", "scipy", "seaborn", "openpyxl"]
    [[fetch]]
    from = 'databases/'
    files = ['correlation_matrices.xlsx']
</py-config>

  <header>
    <nav>
      <ul>
        <li><a href="homepage_neu.html#project-showcase">Projekt-Schaufenster</a></li>
        <li><a class="translation" href="project_3_en.html">English Version</a></li>
      </ul>
    </nav>
  </header>

<section id="homepage">
  <h1>Verschwendungsverst√§ndnis in der Produktion:</h1>
  <h2>Qualit√§tskontrolle und Qualit√§tsvorhersage in einem Eisenschwamm-werk. Wie Interpretationen von Analysen und
      Modellen verwendet werden k√∂nnen, um die Abfallproduktion zu kontrollieren und die Abfallerzeugung vorherzusagen.
  </h2>
    <div class="project-container">
        <h3>NACHHALTIGKEIT hat sich schnell zu einer zentralen Priorit√§t in der modernen Fertigung entwickelt,
            angetrieben durch die wachsende Nachfrage nach umweltfreundlicheren Produkten und einer geringeren
            Belastung der Umwelt. Dies ist auf ein neues Bed√ºrfnis innerhalb der Fertigungsindustrie zur√ºckzuf√ºhren,
            umweltbewusster zu handeln und eine gr√∂√üere Verantwortung f√ºr die Auswirkungen der meisten
            Fertigungsprozesse zu √ºbernehmen.
            <br><br>
            In industriellen Produktionssystemen, insbesondere in energie- und ressourcenintensiven Sektoren, treten
            Abf√§lle in verschiedenen Formen auf, wie z. B. Materialverluste oder Energieverschwendung. Dies macht es
            f√ºr Unternehmen notwendig, digitale L√∂sungen zu entwickeln, um Abf√§lle in Fertigungsprozessen besser zu
            verstehen, indem sie die Einflussfaktoren identifizieren und quantifizieren, die zu Abf√§llen f√ºhren, die
            Produktqualit√§t verbessern und einen profitableren und nachhaltigeren Betrieb f√∂rdern.
            <br><br>
            Die Eisen- und Stahlindustrie steht trotz ihrer technologischen Fortschritte st√§ndig vor diesen
            Herausforderungen. Die Herstellung von Eisenschwamm (auch bekannt als direkt reduziertes Eisen, DRI) ist
            bekannt f√ºr ihre Abf√§lle aufgrund der nichtlinearen und hochkomplexen thermochemischen Reaktionen, die in
            einem riesigen Reaktor unter strengen Temperatur- und Druckbedingungen ablaufen. Daher ist die √úberwachung
            und Steuerung von Qualit√§tsabweichungen und Abf√§llen in Echtzeit eine gro√üe Herausforderung. (UND auch eine
            wichtige Chance f√ºr Datenanalyse und Optimierungsmodellierung)
            <br><br>
            Dank der Integration fortschrittlicher Datenanalyse- und maschineller Lerntechniken in Fertigungssysteme
            ist es nun m√∂glich, Tausende von Sensoren und Messdaten zur Interpretation zu verarbeiten. Im Zusammenhang
            mit der Schwamm-Eisen-Produktion erm√∂glichen diese Tools die Entwicklung von Vorhersagemodellen, die die
            Qualit√§t der Rohstoffe, Temperatur- und Drucksensordaten innerhalb des Direktreduktionsreaktors und die
            Eigenschaften des Endprodukts miteinander verkn√ºpfen.
        </h3>

        <h3>Dies erm√∂glicht einen dynamischen Qualit√§tskontrollkreislauf, in dem Abweichungen antizipiert und
            korrigiert werden k√∂nnen, bevor sie am Ende des Herstellungsprozesses zu Ausschuss f√ºhren.
            <br><br>
            Dieses Projekt untersucht, wie datengest√ºtzte Interpretationen komplexer Prozesse zwei Hauptziele
            unterst√ºtzen k√∂nnen: <b>Qualit√§tskontrolle</b> und <b>Qualit√§tsprognose</b>. Diese Ziele werden anhand von
            simulierten Prozessdaten angegangen, wie sie typischerweise in einer Eisenschwammfabrik vorkommen.
            <br><br>
            Im Rahmen dieses Projekts werden wir die <b>CRISP-DM-Methodik</b> anwenden, um Vorhersagemodelle zu
            erstellen, mit denen sich die Abfallerzeugung prognostizieren l√§sst. Dazu werden Muster anhand von
            Kontrollsensoren in einem Direktreduktionsreaktor und Informationen aus Materialproben identifiziert, die
            vor und nach dem Prozess entnommen wurden. Ausgangspunkt ist das Verst√§ndnis der Gesch√§ftsziele, Endpunkt
            ist die Bewertung des Modells zusammen mit umsetzbaren Erkenntnissen aus den Daten.
            <br><br>
            Im Rahmen dieses Projekts werden wir das Konzept der Nachhaltigkeit in der Fertigung untersuchen und wie
            Interpretationen von Analysen und Modellen zur Kontrolle der Abfallproduktion und zur Vorhersage der
            Abfallerzeugung genutzt werden k√∂nnen. Dabei werden datengest√ºtzte Ans√§tze genutzt, mit denen Hersteller
            die Ursachen f√ºr Abfall identifizieren, ihre Produktionsprozesse optimieren und ihren √∂kologischen
            Fu√üabdruck verringern k√∂nnen.
            <br><br>
            Das √ºbergeordnete Ziel dieses Projekts ist es, aufzuzeigen, wie Hersteller statistische Modelle und
            maschinelles Lernen nutzen k√∂nnen, um Abfallquellen zu identifizieren, Qualit√§tsindikatoren zu √ºberwachen
            und die Ressourcennutzung zu verbessern.
            <br><br>
            <b>HINWEIS:</b> Alle Daten in diesem Projekt sind simuliert und spiegeln nicht die tats√§chlichen Betriebs-
            oder Stichprobendaten wider, die in einer real existierenden Schwamm-Eisen-Produktionsanlage zu finden
            sind. Die am Ende des Projekts gezogenen Schlussfolgerungen stellen keine Ergebnisse dar, die auf eine
            reale Schwamm-Eisen-Produktionsanlage anwendbar sind.

        </h3>

        <h3><b>SCHRITTE 1: Verst√§ndnis der Industrie</b><br><br>
            <b>I. Qualit√§tskontrolle:</b><br>
            Die Qualit√§tskontrolle ist ein wesentlicher Prozess in der Fertigung, der darauf abzielt, Fehler oder
            Abweichungen vom idealen Produktionsprozess zu identifizieren und zu korrigieren. Dieses Projekt wurde
            entwickelt, um Daten von verschiedenen Sensoren zur Qualit√§tskontrolle zu analysieren und Muster von
            Qualit√§tsproblemen unter Ber√ºcksichtigung aller am Fertigungsprozess beteiligten Elemente zu
            identifizieren.
            <br><br>
            Durch die genaue Bestimmung der Ursachen und des Zeitpunkts dieser Abweichungen k√∂nnen in Echtzeit
            Ma√ünahmen ergriffen werden, um die Produktqualit√§t zu stabilisieren und unn√∂tige Material- und
            Energieverschwendung zu reduzieren.
            <br><br>
            <b>II. Qualit√§tsprognose:</b><br>
            Die Qualit√§tsprognose in Fertigungsprozessen nutzt Daten und statistische Modelle, um die Qualit√§t des
            Endprodukts auf der Grundlage der Eingabevariablen und Prozessparameter vorherzusagen. Das Ziel besteht
            darin, potenzielle Qualit√§tsprobleme fr√ºhzeitig im Produktionsprozess zu erkennen, sodass der Hersteller
            die M√∂glichkeit hat, die Produktion vor der Herstellung des Endprodukts zu korrigieren.
            <br><br>
            In einer Eisenschwammfabrik bedeutet dies, die Eigenschaften des Endprodukts (z. B. Porosit√§t,
            Metallisierung, Kohlenstoffgehalt) anhand von Variablen wie der Zusammensetzung des Einsatzmaterials, der
            Reaktortemperatur und dem Druck vorherzusagen. Die Vorhersage der Produktqualit√§t erm√∂glicht proaktive
            Prozessanpassungen, mit denen Verschwendung verhindert werden kann, bevor sie entsteht.
            <br><br>
            Bei den <b>Key Performance Indicators (KPI)</b> ber√ºcksichtigen wir den Mehrwert f√ºr den
            Herstellungsprozess, wenn dieser an die Abfallproduktion angepasst wird. Die KPI sollten zu einer Senkung
            der Kosten, einer Steigerung der Gewinne und einer Verringerung der Umweltbelastung beitragen. (Da Abfall
            in der Regel auf ineffiziente Prozesse, minderwertige Produkte oder eine Kombination aus beidem
            zur√ºckzuf√ºhren ist.)
        </h3>

        <h3><b>SCHRITTE 2: Data Understanding</b><br><br>
            Das Verst√§ndnis der Daten umfasst alle Aktivit√§ten im Zusammenhang mit der Untersuchung der Struktur, des
            Inhalts und der Qualit√§t der Datens√§tze, die f√ºr das Problem relevant sind. Bei diesem Fertigungsprojekt
            liegt der Schwerpunkt in erster Linie auf der Bewertung der Verwendbarkeit der Sensormesswerte,
            Materialproben und Produktionsdaten, die in verschiedenen Phasen des DRI-Prozesses f√ºr Eisenschwamm
            gleichzeitig erfasst werden.
            <br><br>
            Bevor das Projekt fortgesetzt wird, ist es wichtig, den zugrunde liegenden Prozess kurz vorzustellen.
            Eisenschwamm ist ein metallisches Produkt, das durch die direkte Reduktion von Eisenerz in einem
            Direktreduktionsreaktor unter relativ kontrollierten Bedingungen hergestellt wird. Im Gegensatz zu
            Hoch√∂fen, die Koks (eine Art fester Brennstoff) als Reduktionsmittel bei niedrigeren Temperaturen
            verwenden. Der Reduktionsprozess umfasst die Verwendung von Gasen wie Wasserstoff (H<sub>2</sub>).
            <br><br>
            Im DR-Reaktor durchl√§uft das Eisenerz mehrere chemische und thermodynamische Prozesse wie ‚ÄûRedox‚Äù -
            Reaktionen, ‚ÄûGas-Feststoff‚Äù-Reaktionen mit Reduktionsmitteln und W√§rme√ºbertragung. Diese Arten von
            Prozessen umfassen nicht nur relevante Variablen wie Temperatur, Druck und das Vorhandensein chemischer
            Verbindungen, sondern auch physikalische Eigenschaften, die im Eisenerz und im Endprodukt vorhanden sind.
            <br><br>
            Bevor wir Datenmodelle f√ºr die Qualit√§tskontrolle oder Qualit√§tsvorhersage anwenden k√∂nnen, m√ºssen wir
            zun√§chst den Umfang und die Bandbreite der Daten verstehen. In diesem Fall ber√ºcksichtigt das Projekt das
            Produktionsvolumen, Rohstoffdaten, Sensoren im Reaktor und Qualit√§tskontrollaufzeichnungen.
            <br><br>
            <b>HINWEIS:</b> Im Rahmen dieses Projekts werden wir keine Datenqualit√§tsprobleme wie Anomalien oder
            fehlende Daten ber√ºcksichtigen, da alle Daten speziell f√ºr dieses Projekt generiert werden.
        </h3>

        <h3>Die folgenden Hauptdatens√§tze sind in diesem Projekt enthalten:<br><br>
            <b>1. Rohstoffprobendaten:</b> Diese Variablen geben die physikalischen und chemischen Eigenschaften,
            Abmessungen und Zusammensetzung von Eisenerz vor dessen Einbringung in den Direktreduktionsreaktor wieder.
            Daher ist es unerl√§sslich, diese Eingangsvariablen zu verstehen, da sie die Qualit√§t der Reduktion und die
            Bildung von Verunreinigungen im Endprodukt beeinflussen.<br>+ Erzgr√∂√üe (cm)<br>+ Eisenoxidereinheit %
            (Fe<sub>2</sub>O<sub>3</sub>)<br>+ Verunreinigung  1 % (Al<sub>2</sub>O<sub>3</sub>)<br>+ Verunreinigung 2
            % (S)<br>+ Verunreinigung 3 % (CaO)<br>+ Verunreinigung  4 % (C)<br>+ Abrasionsma√ü (mm¬≥)<br>+
            K<sub>2</sub>Cr<sub>2</sub>O<sub>7</sub> Gehalt<br>+ MgO Gehalt<br>+ SiO<sub>2</sub> Gehalt<br>+
            Porosit√§tseinheit % (Œ¶)
        </h3>

        <img src="bilder/project_drei1.png" alt="Project dataset 1">

        <h3><b>2. Reaktorsensoren:</b> Stellen die interne Umgebung des Direktreduktionsreaktors dar, die an
            verschiedenen Stellen mithilfe von Thermoelementen und Manometern √ºberwacht wird, um jeweils die Temperatur
            und den Druck zu messen. Diese Sensoren liefern Erkenntnisse √ºber die Stabilit√§t und Konsistenz der
            chemischen Reaktionen.
            <br>+ Temperaturmessung 1 (C¬∞)<br>+ Druckmessung 1 (Pa)<br>+ Temperaturmessung 2 (C¬∞)
            <br>+ Druckmessung 2 (Pa)<br>+ Temperaturmessung 3 (C¬∞)<br>+ Druckmessung 3 (Pa)
            <br>+ Temperaturmessung 4 (C¬∞)<br>+ Druckmessung 4 (Pa)<br>+ Temperaturmessung 5 (C¬∞)
            <br>+ Druckmessung 5 (Pa)
            <br><br>
            Der Code nimmt eine erneute Stichprobenentnahme dieser Daten in 30-Minuten-Intervallen vor und berechnet
            gleitende Durchschnitte, um die kleinen Schwankungen, die naturgem√§√ü im Reaktor auftreten, zu gl√§tten und
            so reale industrielle √úberwachungssysteme zu simulieren.
        </h3>

        <img src="bilder/project_drei2.png" alt="DR Plant">

        <h3><b>3. Mittelstr√∂me:</b> Umfasst die Zugabe von Reagenzien, chemischen Bindenmitteln und Flussmitteln in den
            Direktreduktionsreaktor, um die Eisenpellets chemisch in Eisenschwamm umzuwandeln. (Die Variablen sind in
            Durchflussraten gemessen.) <br>+ Reagenz 1 H<sub>2</sub> (L/min)<br>+ Reagenz 2 CO (L/min)<br>+ Reagenz 3
            CH<sub>4</sub> (L/min)<br>+ Chemisches Bindemittel 1 Al<sub>2</sub>O<sub>3</sub> (kg/min)<br>+ Chemisches
            Bindemittel  2 4SiO<sub>2</sub>(kg/min)<br>+ Chemisches Bindemittel 3 C<sub>6</sub>H<sub>10</sub>O
            <sub>5</sub> (kg/min)<br>+ Chemisches Bindemittel 4 C<sub>12</sub>H<sub>22</sub>O<sub>11</sub> (kg/min)
            <br>+ Flussmittel 1 CaCO<sub>3</sub> (L/min)<br>+ Flussmittel 2 CaMgCO<sub>3</sub><br>+ Flussmittel 3
            SiO<sub>2</sub>
        </h3>

        <h3>Agentenstr√∂me sind ein sehr wichtiges Element im Direktreduktionsprozess, da jede Art von chemischer
            Komponente eine Funktion f√ºr den Gesamtzustand des Endprodukts hat.
            <br><br>
            Beispielsweise reagieren die <b>Reagenzien</b>, anders bekannt als Reduktionsgase, im
            Direktreduktionsprozess mit dem Eisenoxid (FeO) und bilden dabei Eisenschwamm (Fe), w√§hrend
            <b>Bindemittel</b> Pulver sind, die hinzugef√ºgt werden, um die Fasern zusammenzuhalten und eine koh√§sive
            Struktur der Eisenpellets zu schaffen. Schlie√ülich dienen die <b>Flussmittel</b> dazu, die chemischen
            Reaktionen zu f√∂rdern, die unter bestimmten Umgebungsbedingungen stattfinden.
            <br><br>
            Das Verst√§ndnis der Funktion dieser Verbindungen ist entscheidend, um ihr Flie√üverhalten mit den
            Prozessergebnissen in Verbindung zu bringen. Beispielsweise k√∂nnen Schwankungen im H‚ÇÇ-Fluss die
            Metallisierung beeinflussen, w√§hrend Unregelm√§√üigkeiten in der Bindemittelzusammensetzung zu erh√∂htem
            Abrieb oder zur Entstehung von Abfallpulver f√ºhren k√∂nnen.
        </h3>

        <h3><b>4. DRI Probendaten:</b> dienen zur Analyse der Verteilung zwischen dem Endprodukt und den nach dem
            Direktreduktionsprozess anfallenden Abf√§llen sowie der Qualit√§t des DRI-Endprodukts, die anhand der Arten
            von Verunreinigungen gemessen wird, die √ºblicherweise im Eisenerz vorkommen. <br>+ DRI-Pellet %<br>+
            Pelletgr√∂√üe (cm)<br>+ DRI Reinheit % (Fe)<br>+ Verunreinigung 1 % (S)<br>+ Verunreinigung 2 % (C)<br>+
            Metallisierung %<br>+ Schlacke % <br>+ Abfallpulver %<br>+ Porosit√§tseinheit % (Œ¶)
        </h3>

        <img src="bilder/project_drei3.png" alt="DRI Process">

        <h3><b>SCHRITTE 3: Aufbereitung der Daten</b><br><br>
            Nachdem nun alle Daten erfasst und vorverarbeitet wurden, k√∂nnen wir feststellen, dass jeder Datensatz aus
            einem anderen Messpunkt innerhalb des Direktreduktionsprozesses stammt. Dies spiegelt die Funktionsweise
            tats√§chlicher langsamer industrieller Prozesse wider, bei denen Daten asynchron aus mehreren
            Produktionsstufen generiert werden. Aus diesem Grund muss das Modell die Informationen anpassen, indem es
            die Ausgabedaten mit den verschiedenen Eingabedatenquellen verkn√ºpft.
            <br><br>
            Auf diese Weise kann das Modell die Umwandlung jeder einzelnen Pelletcharge Eisenerz w√§hrend des gesamten
            DRI-Produktionszyklus verfolgen, indem es die Datens√§tze an ihren jeweiligen Zeitpunkten aufeinander
            abstimmt. In einem typischen DRI-Prozess dauert eine vollst√§ndige Umwandlung ‚Äì vom Rohpellet-Einsatz bis
            zum reduzierten Eisenaussto√ü ‚Äì etwa 9 Stunden.
        </h3>

        <section class="sample_code">
            <b># Python CODE: Aggregation von Daten</b>
            <br><b>Im Code wird diese Logik direkt auf den Datenrahmen df_raw_material angewendet, indem 9 Stunden vom
            Zeitstempel abgezogen werden. Dadurch werden die Rohstoffmerkmale mit der entsprechenden DRI-Ausgabe f√ºr
            jede Charge abgeglichen.</b>
            <br><br>try:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df_raw_material['Date'] = pd.to_datetime(df_raw_material['Date'], format='%d/%m/%Y %H:%M')
            <br>except ValueError as e:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df_raw_material['Date'] = pd.to_datetime(df_raw_material['Date'], format='mixed')
            <br>df_raw_material['Date'] = df_raw_material['Date'] - pd.Timedelta(hours=9)
            <br><br><b>Als N√§chstes betrachten wir die Sensordaten, die aus dem Direktreduktionsreaktor (DR) gesammelt
            wurden. Diese Variablen umfassen Druck- und Temperaturmessungen aus verschiedenen Reaktorzonen, die mit
            hoher Frequenz (alle 2 Minuten) aufgezeichnet wurden. Um die Granularit√§t zu reduzieren und die
            Interpretierbarkeit zu verbessern, werden diese Messungen auf 30-Minuten-Intervalle umgerechnet, wobei der
            Mittelwert und die Standardabweichung berechnet werden, um die zentralen Tendenzen, aber auch die
            betriebliche Variabilit√§t zu erfassen.</b>
        </section>

        <h3>Die endg√ºltigen Qualit√§tskennzahlen, die in den DRI-Proben zum Zeitpunkt t beobachtet werden, werden durch
            die Rohstoffe und Betriebsbedingungen beeinflusst, die zum Zeitpunkt t - 9 Stunden vorher herrschten.
            Folglich muss jeder der vier in der Datenverst√§ndnisphase vorgestellten Datens√§tze einer zeitlichen
            Anpassung unterzogen werden, um seine Rolle in diesem 9-st√ºndigen Transformationsfenster widerzuspiegeln.
            <br><br>
            Wenn beispielsweise um 16:00 Uhr eine DRI-Pelletprobe entnommen wird, wurden die f√ºr ihre Bildung
            verantwortlichen Rohstoffeigenschaften um 7:00 Uhr morgens erfasst. Um diese beiden Datens√§tze miteinander
            zu verkn√ºpfen, muss in der Datenaufbereitungsphase der Zeitstempel des Rohstoffdatensatzes um 9 Stunden
            zur√ºckverschoben werden. Diese Anpassung erm√∂glicht es uns, einen kausalen Zusammenhang zwischen den
            Eingabematerialdaten und den Ausgabedaten zur Qualit√§t der DRI-Pellets herzustellen.
        </h3>

        <section class="sample_code">
            <b># Python CODE: Rollierende Fenstertechnik CODE</b>
            <br><br>try:
            resampled_df = df_reactor_sensor.resample('30T').agg({<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 1':
            ['mean', 'std'], 'Pressure 1': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 2': ['mean',
            'std'], 'Pressure 2': ['mean', 'std'], <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 3': ['mean', 'std'],
            'Pressure 3': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 4': ['mean', 'std'], 'Pressure
            4': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 5': ['mean', 'std'], 'Pressure 5':
            ['mean', 'std']})
            <br>resampled_df.reset_index(inplace=True)
            <br><br>
            <b>Die Abstimmung der Sensordaten mit der DRI-Ausgabe ist jedoch komplexer. Da der Reaktor √ºber einen
                Zeitraum von 9-Stunden kontinuierlich Material verarbeitet, sind die relevanten Sensorwerte diejenigen,
                die w√§hrend des gesamten Zeitfensters bis zur Probenahme des DRI beobachtet werden.</b>
            <br><br>
            <b>Um dies zu ber√ºcksichtigen, wird eine rollierende Fenstertechnik auf die Zeitreihen der Sensoren
                angewendet. Der gleitende Durchschnitt und die gleitende Standardabweichung √ºber ein 9-Stunden-Fenster
                werden berechnet, um die kumulative Wirkung der Reaktorbedingungen auf jede Pelletcharge darzustellen.</b>
            <br><br>df_reactor_sensor = df_reactor_sensor.rolling("9H").mean()
            <br>df_reactor_sensor = df_reactor_sensor.iloc[18:]
        </section>

        <h3>Diese Technik wird auch auf den Datensatz zum Wirkstofffluss angewendet. √Ñhnlich wie bei den Sensoren
            variieren die Zusammensetzung und Menge der reduzierenden Gase, Bindemittel und Flussmittel im Laufe der
            Zeit und beeinflussen die chemischen Reaktionen im Reaktor. Durch die Anwendung eines gleitenden Fensters
            auf diesen Datensatz k√∂nnen wir das durchschnittliche Verhalten und die Schwankungen der
            Schl√ºsselkomponenten √ºber denselben Zeitraum von 9 Stunden berechnen.
            <br><br>
            Durch die Verwendung von Zeitverschiebung und rollierender Fensteraggregation wandeln wir die
            urspr√ºnglichen asynchronen Datens√§tze in einen einheitlichen, zeitlich abgestimmten Datensatz um. Jede
            Zeile in der endg√ºltigen Version des zusammengef√ºhrten DataFrame repr√§sentiert nun dasselbe Eisenerz
            w√§hrend des gesamten Prozesses und verkn√ºpft die Eigenschaften des Rohmaterials, den internen Zustand des
            Reaktors w√§hrend der Umwandlung der Pellets, die beteiligten chemischen Wirkstoffe und die endg√ºltigen
            DRI-Qualit√§tsergebnisse.
            <br><br>
            Dank der Aggregation der Reaktordaten stellen wir sicher, dass die Eingabemerkmale und Zielvariablen die
            tats√§chlichen Prozessabh√§ngigkeiten widerspiegeln, wodurch die Relevanz und Genauigkeit unserer
            Qualit√§tsvorhersage- und Abfallsch√§tzungsmodelle verbessert wird. Dies verhindert auch eine Angleichung
            nicht √ºbereinstimmender Beobachtungen.
        </h3>

        <h3><b>STEP 4: Data Exploration and Visualization</b><br><br>
            Now that the data from all sources are completely aligned it is time to apply data exploration and
            visualization techniques in order to understand the data's characteristics, uncover potential patterns, and
            make informed decisions about which variables are relevant for the modeling phase.
            <br><br>
            Due to the fact that we are handling a great number of independent variables (52) it is important for us to
            first examine dimensionality problems that might surface when building our model. This is done because
            <b>Dimensionality reduction</b> improves the interpretability, stability, computational efficiency and
            performance of statistical models and forecasting algorithms.
            <br><br>
            Dimensionality problems arise when multiple variables are highly correlated, which means that they carry
            redundant information. This redundancy can inflate the dimensionality of the dataset without providing new
            insights. Within this project, we will try out two different methods of dimensionality reduction and compare
            the performance of the model, as a way to objectively compare which method offers higher value to the end
            result.
            <br><br>
            The methods that we'll try out during this project will be <b>Reverse Stepwise Regression</b> and
            <b>Principal Component Analysis (PCA)</b> which both require an in-depth analysis of the relationships
            between independent variables as a way to determine which variables offer redundancy within the dataset.
            In this project, we will not only apply basic Pearson correlation metrics but also prove if there are
            non-linear relationships between the variables using Spearman's rank correlation coefficient.
            <br><br>
            <b>Note:</b> Reverse Stepwise Regression and PCA will help in understanding which variables impact the most
            in waste generation <b>(Quality Control)</b> and enable the creation of more accurate models that forecast
            the amount of waste produced <b>(Quality prediction)</b>.
        </h3>

        <section class="sample_code">
            <b>Pearson's correlation matrix</b> measures the linear relationship between two continuous variables, which
            in the case of our dataset, it is applicable to all prepared data. Pearson's correlation is commonly used
            when analyzing relationships between variables with continuous data, assuming that the data is normally
            distributed and that the relationship between all variables is linear.
            <br><br>
            <div id="pearson_heatmap">
                <py-script>
                import pandas as pd
                import seaborn as sns
                import openpyxl
                import matplotlib.pyplot as plt
                from pyscript import display

                df_1 = pd.read_excel('correlation_matrices.xlsx', sheet_name="Pearson Correlation", header=0, index_col=0)

                fig1, ax1 = plt.subplots()
                sns.heatmap(df_1, annot=False, cmap="Blues", ax=ax1, xticklabels=False, yticklabels=False)
                plt.title("Pearson correlation (DRI process)")
                display(fig1, target="pearson_heatmap")
                </py-script>
            </div>

            <br><br>
            <b>Spearman's correlation matrix</b> measures instead the strength and direction of a
            monotonic relationship between two variables. Pearson's correlation coefficient does not assume linearity
            between the variables, nor does it assume that the data is normally distributed, which is particularly
            helpful if there are exponential, logarithmic or even polynomial relationships between two variables.
            <br><br>
            <div id="spearman_heatmap">
                <py-script>
                import pandas as pd
                import seaborn as sns
                import openpyxl
                import matplotlib.pyplot as plt
                from pyscript import display

                df_2 = pd.read_excel('correlation_matrices.xlsx', sheet_name="Spearman Correlation", header=0, index_col=0)

                fig2, ax2 = plt.subplots()
                sns.heatmap(df_2, annot=False, cmap="Greens", ax=ax2, xticklabels=False, yticklabels=False)
                plt.title("Spearman correlation (DRI process)")
                display(fig2, target="spearman_heatmap")
                </py-script>
            </div>
        </section>

        <h3><b>STEP 5: Feature Engineering</b><br><br>
            For "Reverse Stepwise Regression", the goal is to remove the least significant predictor (based on p-value)
            at each step, by fitting the complete dataset into a model to the data, evaluate its performance and then
            slowly remove all predictors that have the least significance until the performance of the model reaches
            it's peak.
            <br><br>
            On the other hand, Principal Component Analysis (PCA) is built by transforming the dataset into principal
            components, through the use of <b>Eigendecomposition</b> which decomposes the group of dependent
            variables into their respective eigenvectors and eigenvalues. PCA then selects the top Principal components
            by picking out eigenvectors based on their corresponding eigenvalues.
            <br><br>
            For both cases, we will have to preprocess the dataset by scaling all data equally. This is done because
            most of the independent variables don't share the same metrics (Pressures, Temperatures, Percentages,
            Measurements, etc.). There are many different scaling methods from the sklearn.preprocessing library like
            StandardScaler, MinMaxScaler, RobustScaler, etc. that have the same goal of standardizing data.
        </h3>

        <section class="sample_code">
            <b># Python CODE: MinMaxScaling<br>
            After applying both "Shapiro-Wilk" and "Kolmogorov-Smirnov" tests for normality and goodness of fit. We can
            determine that none of the independent variables follow a normal distribution which pushes us to use the
            MinMaxScaler as it's particularly effective when the data is not normally distributed, by applying a
            scaling method that doesn't assume any distribution and scales the data to a range of [-1, 1], preserving
            the original non-normal distribution.<br>
            Note: MinMaxScaler applies a uniform scaling by subtracting the minimum value and dividing by the range of
            the dataset.</b><br>
            <br>from sklearn.preprocessing import MinMaxScaler
            <br>scaler = MinMaxScaler()
            <br>independent_variables = final_variables.iloc[:, 1:].copy()
            <br>independent_variables.columns = independent_variables.columns.astype(str)
            <br>independent_variables = scaler.fit_transform(independent_variables)<br><br>
            <b># Python CODE: Principal Component Analysis (PCA)<br>
            In scikit-learn, PCA is implemented as a transformer object that learns components in its fit() function,
            which computes the covariance matrix, handles the eigendecomposition on the covariance matrix and then
            generates the principal components.</b><br>
            <br>from sklearn.decomposition import PCA
            <br>pca_dri = PCA()
            <br>pca_dri.fit(independent_variables)
            <br>principal_components = pca_dri.transform(independent_variables)
            <br>selected_components = principal_components[:, :6]
            <br><br><b>The PCA components will be later fed into a General Additive Model (GAM) regression model that
            will consider the complexity of the relationships between the independent and dependent variables, as well
            as offer an in-depth statistical analysis of the entire process.</b>
        </section>

        <h3>Thanks to the explained_variance_ratio_ function within sklearn.decomposition we can measure the percentages
            of variance explained by each of generated principal components. By calculating the accumulated sum of the
            ratios we can determine that the total variance of the first 6 PCA components reach a variance of 92.39%.
            (If we handle 5 Components the percentage is 87.97% and if we handle 7 PCA components the variance reaches
            93.61%)<br><br>
            <b>Note:</b> If we use 14 or more components, the explained_variance_ratio_ is 1.0 indicating that we‚Äôve
            captured 100% of the variance in the dataset.
        </h3>

        <video id="video_1" controls autoplay loop muted>
            <source src="videos/project_drei.mp4" type="video/mp4">
        </video>

        <h3><b>STEP 6: Model Building and Training</b><br><br>
            As mentioned before, the General Additive Model (GAM) regression model serves as a way to
            estimate the relationships between the dependent variables of the DR process and the independent variables
            that represent the qualities of the output obtained by the process. GAM is a generalized linear model which
            formulates a linear response depending on multiple smooth functions (which for the case of this problem are
            splines).
            <br><br>
            When combined with PCA, GAM can efficiently handle high-dimensional data by using the principal components
            as input features which are then adapted into individual smoothing splines (regression splines involve
            dividing the range of the PCA component into <b>K</b> distinct regions where a polynomial function is then
            applied.
        </h3>

        <img src="bilder/project_drei4.png" alt="Project Scatter Graph 1">

        <img src="bilder/project_drei5.png" alt="Project Scatter Graph 2">

        <h3>The GAM model for this project adapts the following form s(0) + s(1) + ... + s(5), where all 6 main PCA
            components are molded into smoothing splines. The GAM model is then adapted to handle all 9 dependent
            variables that represent the Direct Iron Ore that is generated by the manufacturing process. This is done by
            fitting a separate GAM model for each target variable using the selected principal components as input.
            <br><br>
            For this reason, are scatter plots the best option for visually comparing the actual observed values
            (represented by the scatter points) with the predicted values generated by the GAM model (represented by
            the line plot). This comparison helps in evaluating how well the model fits the data and captures the
            underlying patterns.
        </h3>

        <h3><b>NOTE:</b> By comparing the values of the first Principal Component, we can capture the most significant
            patterns or variations in the data between the main patterns in the data and the target variable.
            <br><br>
            Now that the <b>(Quality prediction)</b> model is complete, it's time to use the Reverse Stepwise Regression
            in order to determine the main 6 variables that will later be used for the same structured GAM model made
            for the Principal Components.
            <br><br>
            This can be done with the statsmodel library, which applies regression by fitting all of the variables and
            then measuring their respective p-values. Once the p-values have been calculated, the variable with the
            lowest p-value is then removed and the process starts all over again.
        </h3>

        <h3><b>STEP 7: Model Evaluation and Comparison</b><br><br>
            The GCV values indicate a notable difference in predictive error, with the PCA model achieving a minimal
            value of 0.0001, suggesting an extremely low prediction error. In contrast, the Reverse Stepwise Regression
            model shows a GCV of 0.0, which might imply overfitting.
            <br><br>
            When examining the AIC values and Pseudo R-Squared values, PCA model achieves a better balance of fit and
            complexity by showing a significantly lower AIC and achieves a Pseudo R-Squared of 0.3382, suggesting that
            about 33.82% of the variability in the outcome can be explained by this model. Conversely, the Reverse
            Stepwise Regression model boasts a much higher Pseudo R-Squared of 0.8252.
            <br><br>
            In conclusion, while the Reverse Stepwise Regression model excels in explanatory power as reflected by its
            high Pseudo R-Squared value, the PCA model demonstrates better overall performance when considering the
            balance of prediction accuracy and model simplicity, as indicated by its lower AIC.
        </h3>

        <h3><b>STEP 8: Model Refinement</b><br><br>
            By comparing the result of the GAM models that use PCA with the models that use only the statistically
            relevant variables obtained from the Reverse Stepwise Reduction technique, we can see that even though both
            are mathematical techniques specialized for dimension reduction, there is considerably more value lost in
            one technique than the other.
            <br><br>
            Some aspects that we have to value from the PCA models are their inherent ability to enhance the performance
            of the GAM models in a way where there is no value lost that would otherwise be discarded from the reverse
            stepwise regression technique.
            <br><br>
            At the end of the day, even though one model offers a better fit than the other, we should always consider
            the computational costs that arise when handling with large quantities of features (which is extremely
            common in a manufacturing process).
        </h3>

        <h3><b>Conclusion:</b><br>
            The use of GAM models in manufacturing processes like Direct Reduction could bring plenty of value in the
            reduction of waste (Quality Control) and the forecasting of end results (Quality Prediction), thanks to the
            insights obtained from the analysis of PRINCIPAL COMPONENTS and/or STATISTICALLY RELEVANT variables.
            <br><br>
            This insight facilitates root cause analysis and enables targeted interventions to improve the performance
            of the process and the quality of it's final product. All thanks to the examination of the estimated
            coefficients and smooth functions in the GAM model, manufacturers can pinpoint which variables have the
            most significant impact on product quality and how they interact with one another.
        </h3>
    </div>
</section>


  <footer>
    <p>Erstellt von Adri√°n Ochoa Ferri√±o - 2023</p>
  </footer>

</body>
</html>