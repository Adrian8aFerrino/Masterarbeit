<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22>
    <text y=%22.9em%22 font-size=%2290%22>üê∏</text></svg>">
    <meta name="description" content="Quality control and prediction" />
    <meta name="author" content="Adri√°n Ochoa Ferri√±o" />
    <title>Projekt 3: Verschwendungsverst√§ndnis in der Produktion</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://pyscript.net/releases/2025.3.1/core.css">
    <script type="module" src="https://pyscript.net/releases/2025.3.1/core.js"></script>
</head>

<body>
<py-config>
    packages = ["pandas", "scipy", "seaborn", "openpyxl"]
    [[fetch]]
    from = 'databases/'
    files = ['correlation_matrices.xlsx']
</py-config>

  <header>
    <nav>
      <ul>
        <li><a href="homepage_neu.html#project-showcase">Projekt-Schaufenster</a></li>
        <li><a class="translation" href="project_3_en.html">English Version</a></li>
      </ul>
    </nav>
  </header>

<section id="homepage">
  <h1>Verschwendungsverst√§ndnis in der Produktion:</h1>
  <h2>Qualit√§tskontrolle und Qualit√§tsvorhersage in einem Eisenschwamm-werk. Wie Interpretationen von Analysen und
      Modellen verwendet werden k√∂nnen, um die Abfallproduktion zu kontrollieren und die Abfallerzeugung vorherzusagen.
  </h2>
    <div class="project-container">
        <h3>NACHHALTIGKEIT hat sich schnell zu einer zentralen Priorit√§t in der modernen Fertigung entwickelt,
            angetrieben durch die wachsende Nachfrage nach umweltfreundlicheren Produkten und einer geringeren
            Belastung der Umwelt. Dies ist auf ein neues Bed√ºrfnis innerhalb der Fertigungsindustrie zur√ºckzuf√ºhren,
            umweltbewusster zu handeln und eine gr√∂√üere Verantwortung f√ºr die Auswirkungen der meisten
            Fertigungsprozesse zu √ºbernehmen.
            <br><br>
            In industriellen Produktionssystemen, insbesondere in energie- und ressourcenintensiven Sektoren, treten
            Abf√§lle in verschiedenen Formen auf, wie z. B. Materialverluste oder Energieverschwendung. Dies macht es
            f√ºr Unternehmen notwendig, digitale L√∂sungen zu entwickeln, um Abf√§lle in Fertigungsprozessen besser zu
            verstehen, indem sie die Einflussfaktoren identifizieren und quantifizieren, die zu Abf√§llen f√ºhren, die
            Produktqualit√§t verbessern und einen profitableren und nachhaltigeren Betrieb f√∂rdern.
            <br><br>
            Die Eisen- und Stahlindustrie steht trotz ihrer technologischen Fortschritte st√§ndig vor diesen
            Herausforderungen. Die Herstellung von Eisenschwamm (auch bekannt als direkt reduziertes Eisen, DRI) ist
            bekannt f√ºr ihre Abf√§lle aufgrund der nichtlinearen und hochkomplexen thermochemischen Reaktionen, die in
            einem riesigen Reaktor unter strengen Temperatur- und Druckbedingungen ablaufen. Daher ist die √úberwachung
            und Steuerung von Qualit√§tsabweichungen und Abf√§llen in Echtzeit eine gro√üe Herausforderung. (UND auch eine
            wichtige Chance f√ºr Datenanalyse und Optimierungsmodellierung)
            <br><br>
            Dank der Integration fortschrittlicher Datenanalyse- und maschineller Lerntechniken in Fertigungssysteme
            ist es nun m√∂glich, Tausende von Sensoren und Messdaten zur Interpretation zu verarbeiten. Im Zusammenhang
            mit der Schwamm-Eisen-Produktion erm√∂glichen diese Tools die Entwicklung von Vorhersagemodellen, die die
            Qualit√§t der Rohstoffe, Temperatur- und Drucksensordaten innerhalb des Direktreduktionsreaktors und die
            Eigenschaften des Endprodukts miteinander verkn√ºpfen.
        </h3>

        <h3>Dies erm√∂glicht einen dynamischen Qualit√§tskontrollkreislauf, in dem Abweichungen antizipiert und
            korrigiert werden k√∂nnen, bevor sie am Ende des Herstellungsprozesses zu Ausschuss f√ºhren.
            <br><br>
            Dieses Projekt untersucht, wie datengest√ºtzte Interpretationen komplexer Prozesse zwei Hauptziele
            unterst√ºtzen k√∂nnen: <b>Qualit√§tskontrolle</b> und <b>Qualit√§tsprognose</b>. Diese Ziele werden anhand von
            simulierten Prozessdaten angegangen, wie sie typischerweise in einer Eisenschwammfabrik vorkommen.
            <br><br>
            Im Rahmen dieses Projekts werden wir die <b>CRISP-DM-Methodik</b> anwenden, um Vorhersagemodelle zu
            erstellen, mit denen sich die Abfallerzeugung prognostizieren l√§sst. Dazu werden Muster anhand von
            Kontrollsensoren in einem Direktreduktionsreaktor und Informationen aus Materialproben identifiziert, die
            vor und nach dem Prozess entnommen wurden. Ausgangspunkt ist das Verst√§ndnis der Gesch√§ftsziele, Endpunkt
            ist die Bewertung des Modells zusammen mit umsetzbaren Erkenntnissen aus den Daten.
            <br><br>
            Im Rahmen dieses Projekts werden wir das Konzept der Nachhaltigkeit in der Fertigung untersuchen und wie
            Interpretationen von Analysen und Modellen zur Kontrolle der Abfallproduktion und zur Vorhersage der
            Abfallerzeugung genutzt werden k√∂nnen. Dabei werden datengest√ºtzte Ans√§tze genutzt, mit denen Hersteller
            die Ursachen f√ºr Abfall identifizieren, ihre Produktionsprozesse optimieren und ihren √∂kologischen
            Fu√üabdruck verringern k√∂nnen.
            <br><br>
            Das √ºbergeordnete Ziel dieses Projekts ist es, aufzuzeigen, wie Hersteller statistische Modelle und
            maschinelles Lernen nutzen k√∂nnen, um Abfallquellen zu identifizieren, Qualit√§tsindikatoren zu √ºberwachen
            und die Ressourcennutzung zu verbessern.
            <br><br>
            <b>HINWEIS:</b> Alle Daten in diesem Projekt sind simuliert und spiegeln nicht die tats√§chlichen Betriebs-
            oder Stichprobendaten wider, die in einer real existierenden Schwamm-Eisen-Produktionsanlage zu finden
            sind. Die am Ende des Projekts gezogenen Schlussfolgerungen stellen keine Ergebnisse dar, die auf eine
            reale Schwamm-Eisen-Produktionsanlage anwendbar sind.

        </h3>

        <h3><b>SCHRITT 1: Verst√§ndnis der Industrie</b><br><br>
            <b>I. Qualit√§tskontrolle:</b><br>
            Die Qualit√§tskontrolle ist ein wesentlicher Prozess in der Fertigung, der darauf abzielt, Fehler oder
            Abweichungen vom idealen Produktionsprozess zu identifizieren und zu korrigieren. Dieses Projekt wurde
            entwickelt, um Daten von verschiedenen Sensoren zur Qualit√§tskontrolle zu analysieren und Muster von
            Qualit√§tsproblemen unter Ber√ºcksichtigung aller am Fertigungsprozess beteiligten Elemente zu
            identifizieren.
            <br><br>
            Durch die genaue Bestimmung der Ursachen und des Zeitpunkts dieser Abweichungen k√∂nnen in Echtzeit
            Ma√ünahmen ergriffen werden, um die Produktqualit√§t zu stabilisieren und unn√∂tige Material- und
            Energieverschwendung zu reduzieren.
            <br><br>
            <b>II. Qualit√§tsprognose:</b><br>
            Die Qualit√§tsprognose in Fertigungsprozessen nutzt Daten und statistische Modelle, um die Qualit√§t des
            Endprodukts auf der Grundlage der Eingabevariablen und Prozessparameter vorherzusagen. Das Ziel besteht
            darin, potenzielle Qualit√§tsprobleme fr√ºhzeitig im Produktionsprozess zu erkennen, sodass der Hersteller
            die M√∂glichkeit hat, die Produktion vor der Herstellung des Endprodukts zu korrigieren.
            <br><br>
            In einer Eisenschwammfabrik bedeutet dies, die Eigenschaften des Endprodukts (z. B. Porosit√§t,
            Metallisierung, Kohlenstoffgehalt) anhand von Variablen wie der Zusammensetzung des Einsatzmaterials, der
            Reaktortemperatur und dem Druck vorherzusagen. Die Vorhersage der Produktqualit√§t erm√∂glicht proaktive
            Prozessanpassungen, mit denen Verschwendung verhindert werden kann, bevor sie entsteht.
            <br><br>
            Bei den <b>Key Performance Indicators (KPI)</b> ber√ºcksichtigen wir den Mehrwert f√ºr den
            Herstellungsprozess, wenn dieser an die Abfallproduktion angepasst wird. Die KPI sollten zu einer Senkung
            der Kosten, einer Steigerung der Gewinne und einer Verringerung der Umweltbelastung beitragen. (Da Abfall
            in der Regel auf ineffiziente Prozesse, minderwertige Produkte oder eine Kombination aus beidem
            zur√ºckzuf√ºhren ist.)
        </h3>

        <h3><b>SCHRITT 2: Data Understanding</b><br><br>
            Das Verst√§ndnis der Daten umfasst alle Aktivit√§ten im Zusammenhang mit der Untersuchung der Struktur, des
            Inhalts und der Qualit√§t der Datens√§tze, die f√ºr das Problem relevant sind. Bei diesem Fertigungsprojekt
            liegt der Schwerpunkt in erster Linie auf der Bewertung der Verwendbarkeit der Sensormesswerte,
            Materialproben und Produktionsdaten, die in verschiedenen Phasen des DRI-Prozesses f√ºr Eisenschwamm
            gleichzeitig erfasst werden.
            <br><br>
            Bevor das Projekt fortgesetzt wird, ist es wichtig, den zugrunde liegenden Prozess kurz vorzustellen.
            Eisenschwamm ist ein metallisches Produkt, das durch die direkte Reduktion von Eisenerz in einem
            Direktreduktionsreaktor unter relativ kontrollierten Bedingungen hergestellt wird. Im Gegensatz zu
            Hoch√∂fen, die Koks (eine Art fester Brennstoff) als Reduktionsmittel bei niedrigeren Temperaturen
            verwenden. Der Reduktionsprozess umfasst die Verwendung von Gasen wie Wasserstoff (H<sub>2</sub>).
            <br><br>
            Im DR-Reaktor durchl√§uft das Eisenerz mehrere chemische und thermodynamische Prozesse wie ‚ÄûRedox‚Äù -
            Reaktionen, ‚ÄûGas-Feststoff‚Äù-Reaktionen mit Reduktionsmitteln und W√§rme√ºbertragung. Diese Arten von
            Prozessen umfassen nicht nur relevante Variablen wie Temperatur, Druck und das Vorhandensein chemischer
            Verbindungen, sondern auch physikalische Eigenschaften, die im Eisenerz und im Endprodukt vorhanden sind.
            <br><br>
            Bevor wir Datenmodelle f√ºr die Qualit√§tskontrolle oder Qualit√§tsvorhersage anwenden k√∂nnen, m√ºssen wir
            zun√§chst den Umfang und die Bandbreite der Daten verstehen. In diesem Fall ber√ºcksichtigt das Projekt das
            Produktionsvolumen, Rohstoffdaten, Sensoren im Reaktor und Qualit√§tskontrollaufzeichnungen.
            <br><br>
            <b>HINWEIS:</b> Im Rahmen dieses Projekts werden wir keine Datenqualit√§tsprobleme wie Anomalien oder
            fehlende Daten ber√ºcksichtigen, da alle Daten speziell f√ºr dieses Projekt generiert werden.
        </h3>

        <h3>Die folgenden Hauptdatens√§tze sind in diesem Projekt enthalten:<br><br>
            <b>1. Rohstoffprobendaten:</b> Diese Variablen geben die physikalischen und chemischen Eigenschaften,
            Abmessungen und Zusammensetzung von Eisenerz vor dessen Einbringung in den Direktreduktionsreaktor wieder.
            Daher ist es unerl√§sslich, diese Eingangsvariablen zu verstehen, da sie die Qualit√§t der Reduktion und die
            Bildung von Verunreinigungen im Endprodukt beeinflussen.<br>+ Erzgr√∂√üe (cm)<br>+ Eisenoxidereinheit %
            (Fe<sub>2</sub>O<sub>3</sub>)<br>+ Verunreinigung  1 % (Al<sub>2</sub>O<sub>3</sub>)<br>+ Verunreinigung 2
            % (S)<br>+ Verunreinigung 3 % (CaO)<br>+ Verunreinigung  4 % (C)<br>+ Abrasionsma√ü (mm¬≥)<br>+
            K<sub>2</sub>Cr<sub>2</sub>O<sub>7</sub> Gehalt<br>+ MgO Gehalt<br>+ SiO<sub>2</sub> Gehalt<br>+
            Porosit√§tseinheit % (Œ¶)
        </h3>

        <img src="bilder/project_drei1.png" alt="Project dataset 1">

        <h3><b>2. Reaktorsensoren:</b> Stellen die interne Umgebung des Direktreduktionsreaktors dar, die an
            verschiedenen Stellen mithilfe von Thermoelementen und Manometern √ºberwacht wird, um jeweils die Temperatur
            und den Druck zu messen. Diese Sensoren liefern Erkenntnisse √ºber die Stabilit√§t und Konsistenz der
            chemischen Reaktionen.
            <br>+ Temperaturmessung 1 (C¬∞)<br>+ Druckmessung 1 (Pa)<br>+ Temperaturmessung 2 (C¬∞)
            <br>+ Druckmessung 2 (Pa)<br>+ Temperaturmessung 3 (C¬∞)<br>+ Druckmessung 3 (Pa)
            <br>+ Temperaturmessung 4 (C¬∞)<br>+ Druckmessung 4 (Pa)<br>+ Temperaturmessung 5 (C¬∞)
            <br>+ Druckmessung 5 (Pa)
            <br><br>
            Der Code nimmt eine erneute Stichprobenentnahme dieser Daten in 30-Minuten-Intervallen vor und berechnet
            gleitende Durchschnitte, um die kleinen Schwankungen, die naturgem√§√ü im Reaktor auftreten, zu gl√§tten und
            so reale industrielle √úberwachungssysteme zu simulieren.
        </h3>

        <img src="bilder/project_drei2.png" alt="DR Plant">

        <h3><b>3. Mittelstr√∂me:</b> Umfasst die Zugabe von Reagenzien, chemischen Bindenmitteln und Flussmitteln in den
            Direktreduktionsreaktor, um die Eisenpellets chemisch in Eisenschwamm umzuwandeln. (Die Variablen sind in
            Durchflussraten gemessen.) <br>+ Reagenz 1 H<sub>2</sub> (L/min)<br>+ Reagenz 2 CO (L/min)<br>+ Reagenz 3
            CH<sub>4</sub> (L/min)<br>+ Chemisches Bindemittel 1 Al<sub>2</sub>O<sub>3</sub> (kg/min)<br>+ Chemisches
            Bindemittel  2 4SiO<sub>2</sub>(kg/min)<br>+ Chemisches Bindemittel 3 C<sub>6</sub>H<sub>10</sub>O
            <sub>5</sub> (kg/min)<br>+ Chemisches Bindemittel 4 C<sub>12</sub>H<sub>22</sub>O<sub>11</sub> (kg/min)
            <br>+ Flussmittel 1 CaCO<sub>3</sub> (L/min)<br>+ Flussmittel 2 CaMgCO<sub>3</sub><br>+ Flussmittel 3
            SiO<sub>2</sub>
        </h3>

        <h3>Agentenstr√∂me sind ein sehr wichtiges Element im Direktreduktionsprozess, da jede Art von chemischer
            Komponente eine Funktion f√ºr den Gesamtzustand des Endprodukts hat.
            <br><br>
            Beispielsweise reagieren die <b>Reagenzien</b>, anders bekannt als Reduktionsgase, im
            Direktreduktionsprozess mit dem Eisenoxid (FeO) und bilden dabei Eisenschwamm (Fe), w√§hrend
            <b>Bindemittel</b> Pulver sind, die hinzugef√ºgt werden, um die Fasern zusammenzuhalten und eine koh√§sive
            Struktur der Eisenpellets zu schaffen. Schlie√ülich dienen die <b>Flussmittel</b> dazu, die chemischen
            Reaktionen zu f√∂rdern, die unter bestimmten Umgebungsbedingungen stattfinden.
            <br><br>
            Das Verst√§ndnis der Funktion dieser Verbindungen ist entscheidend, um ihr Flie√üverhalten mit den
            Prozessergebnissen in Verbindung zu bringen. Beispielsweise k√∂nnen Schwankungen im H‚ÇÇ-Fluss die
            Metallisierung beeinflussen, w√§hrend Unregelm√§√üigkeiten in der Bindemittelzusammensetzung zu erh√∂htem
            Abrieb oder zur Entstehung von Abfallpulver f√ºhren k√∂nnen.
        </h3>

        <h3><b>4. DRI Probendaten:</b> dienen zur Analyse der Verteilung zwischen dem Endprodukt und den nach dem
            Direktreduktionsprozess anfallenden Abf√§llen sowie der Qualit√§t des DRI-Endprodukts, die anhand der Arten
            von Verunreinigungen gemessen wird, die √ºblicherweise im Eisenerz vorkommen. <br>+ DRI-Pellet %<br>+
            Pelletgr√∂√üe (cm)<br>+ DRI Reinheit % (Fe)<br>+ Verunreinigung 1 % (S)<br>+ Verunreinigung 2 % (C)<br>+
            Metallisierung %<br>+ Schlacke % <br>+ Abfallpulver %<br>+ Porosit√§tseinheit % (Œ¶)
        </h3>

        <img src="bilder/project_drei3.png" alt="DRI Process">

        <h3><b>SCHRITT 3: Aufbereitung der Daten</b><br><br>
            Nachdem nun alle Daten erfasst und vorverarbeitet wurden, k√∂nnen wir feststellen, dass jeder Datensatz aus
            einem anderen Messpunkt innerhalb des Direktreduktionsprozesses stammt. Dies spiegelt die Funktionsweise
            tats√§chlicher langsamer industrieller Prozesse wider, bei denen Daten asynchron aus mehreren
            Produktionsstufen generiert werden. Aus diesem Grund muss das Modell die Informationen anpassen, indem es
            die Ausgabedaten mit den verschiedenen Eingabedatenquellen verkn√ºpft.
            <br><br>
            Auf diese Weise kann das Modell die Umwandlung jeder einzelnen Pelletcharge Eisenerz w√§hrend des gesamten
            DRI-Produktionszyklus verfolgen, indem es die Datens√§tze an ihren jeweiligen Zeitpunkten aufeinander
            abstimmt. In einem typischen DRI-Prozess dauert eine vollst√§ndige Umwandlung ‚Äì vom Rohpellet-Einsatz bis
            zum reduzierten Eisenaussto√ü ‚Äì etwa 9 Stunden.
        </h3>

        <section class="sample_code">
            <b># Python CODE: Aggregation von Daten</b>
            <br><b>Im Code wird diese Logik direkt auf den Datenrahmen df_raw_material angewendet, indem 9 Stunden vom
            Zeitstempel abgezogen werden. Dadurch werden die Rohstoffmerkmale mit der entsprechenden DRI-Ausgabe f√ºr
            jede Charge abgeglichen.</b>
            <br><br>try:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df_raw_material['Date'] = pd.to_datetime(df_raw_material['Date'], format='%d/%m/%Y %H:%M')
            <br>except ValueError as e:
            <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df_raw_material['Date'] = pd.to_datetime(df_raw_material['Date'], format='mixed')
            <br>df_raw_material['Date'] = df_raw_material['Date'] - pd.Timedelta(hours=9)
            <br><br><b>Als N√§chstes betrachten wir die Sensordaten, die aus dem Direktreduktionsreaktor (DR) gesammelt
            wurden. Diese Variablen umfassen Druck- und Temperaturmessungen aus verschiedenen Reaktorzonen, die mit
            hoher Frequenz (alle 2 Minuten) aufgezeichnet wurden. Um die Granularit√§t zu reduzieren und die
            Interpretierbarkeit zu verbessern, werden diese Messungen auf 30-Minuten-Intervalle umgerechnet, wobei der
            Mittelwert und die Standardabweichung berechnet werden, um die zentralen Tendenzen, aber auch die
            betriebliche Variabilit√§t zu erfassen.</b>
        </section>

        <h3>Die endg√ºltigen Qualit√§tskennzahlen, die in den DRI-Proben zum Zeitpunkt t beobachtet werden, werden durch
            die Rohstoffe und Betriebsbedingungen beeinflusst, die zum Zeitpunkt t - 9 Stunden vorher herrschten.
            Folglich muss jeder der vier in der Datenverst√§ndnisphase vorgestellten Datens√§tze einer zeitlichen
            Anpassung unterzogen werden, um seine Rolle in diesem 9-st√ºndigen Transformationsfenster widerzuspiegeln.
            <br><br>
            Wenn beispielsweise um 16:00 Uhr eine DRI-Pelletprobe entnommen wird, wurden die f√ºr ihre Bildung
            verantwortlichen Rohstoffeigenschaften um 7:00 Uhr morgens erfasst. Um diese beiden Datens√§tze miteinander
            zu verkn√ºpfen, muss in der Datenaufbereitungsphase der Zeitstempel des Rohstoffdatensatzes um 9 Stunden
            zur√ºckverschoben werden. Diese Anpassung erm√∂glicht es uns, einen kausalen Zusammenhang zwischen den
            Eingabematerialdaten und den Ausgabedaten zur Qualit√§t der DRI-Pellets herzustellen.
        </h3>

        <section class="sample_code">
            <b># Python CODE: Rollierende Fenstertechnik CODE</b>
            <br><br>try:
            resampled_df = df_reactor_sensor.resample('30T').agg({<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 1':
            ['mean', 'std'], 'Pressure 1': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 2': ['mean',
            'std'], 'Pressure 2': ['mean', 'std'], <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 3': ['mean', 'std'],
            'Pressure 3': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 4': ['mean', 'std'], 'Pressure
            4': ['mean', 'std'],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'Temperature 5': ['mean', 'std'], 'Pressure 5':
            ['mean', 'std']})
            <br>resampled_df.reset_index(inplace=True)
            <br><br>
            <b>Die Abstimmung der Sensordaten mit der DRI-Ausgabe ist jedoch komplexer. Da der Reaktor √ºber einen
                Zeitraum von 9-Stunden kontinuierlich Material verarbeitet, sind die relevanten Sensorwerte diejenigen,
                die w√§hrend des gesamten Zeitfensters bis zur Probenahme des DRI beobachtet werden.</b>
            <br><br>
            <b>Um dies zu ber√ºcksichtigen, wird eine rollierende Fenstertechnik auf die Zeitreihen der Sensoren
                angewendet. Der gleitende Durchschnitt und die gleitende Standardabweichung √ºber ein 9-Stunden-Fenster
                werden berechnet, um die kumulative Wirkung der Reaktorbedingungen auf jede Pelletcharge darzustellen.</b>
            <br><br>df_reactor_sensor = df_reactor_sensor.rolling("9H").mean()
            <br>df_reactor_sensor = df_reactor_sensor.iloc[18:]
        </section>

        <h3>Diese Technik wird auch auf den Datensatz zum Wirkstofffluss angewendet. √Ñhnlich wie bei den Sensoren
            variieren die Zusammensetzung und Menge der reduzierenden Gase, Bindemittel und Flussmittel im Laufe der
            Zeit und beeinflussen die chemischen Reaktionen im Reaktor. Durch die Anwendung eines gleitenden Fensters
            auf diesen Datensatz k√∂nnen wir das durchschnittliche Verhalten und die Schwankungen der
            Schl√ºsselkomponenten √ºber denselben Zeitraum von 9 Stunden berechnen.
            <br><br>
            Durch die Verwendung von Zeitverschiebung und rollierender Fensteraggregation wandeln wir die
            urspr√ºnglichen asynchronen Datens√§tze in einen einheitlichen, zeitlich abgestimmten Datensatz um. Jede
            Zeile in der endg√ºltigen Version des zusammengef√ºhrten DataFrame repr√§sentiert nun dasselbe Eisenerz
            w√§hrend des gesamten Prozesses und verkn√ºpft die Eigenschaften des Rohmaterials, den internen Zustand des
            Reaktors w√§hrend der Umwandlung der Pellets, die beteiligten chemischen Wirkstoffe und die endg√ºltigen
            DRI-Qualit√§tsergebnisse.
            <br><br>
            Dank der Aggregation der Reaktordaten stellen wir sicher, dass die Eingabemerkmale und Zielvariablen die
            tats√§chlichen Prozessabh√§ngigkeiten widerspiegeln, wodurch die Relevanz und Genauigkeit unserer
            Qualit√§tsvorhersage- und Abfallsch√§tzungsmodelle verbessert wird. Dies verhindert auch eine Angleichung
            nicht √ºbereinstimmender Beobachtungen.
        </h3>

        <h3><b>SCHRITT 4: Datenexploration und -visualisierung</b><br><br>
            Nachdem alle Datens√§tze zeitlich abgeglichen und zu einer einheitlichen Struktur zusammengef√ºhrt wurden,
            k√∂nnen wir nun mit der Datenexploration und -visualisierung fortfahren. Dabei kommen verschiedene
            statistische und Visualisierungstechniken zum Einsatz, die dazu dienen, die statistischen Eigenschaften
            aller oben genannten Variablen zu verstehen, Zusammenh√§nge aufzudecken, Trends und Anomalien zu
            identifizieren sowie eine fundierte Variablenauswahl f√ºr die Modellierungsphase vorzubereiten.
            <br><br>
            Angesichts der hohen Anzahl unabh√§ngiger Variablen im neu strukturierten Datensatz (52 unabh√§ngige
            Variablen) ist es logisch, dass das Modell mit Problemen im Zusammenhang mit hoher Dimensionalit√§t
            konfrontiert sein k√∂nnte. Datens√§tze mit hoher Dimensionalit√§t stellen mehrere Herausforderungen dar: Sie
            k√∂nnen zu √úberanpassung, erh√∂htem Rechenaufwand und unn√∂tiger Modellkomplexit√§t f√ºhren, was die
            Interpretierbarkeit und Robustheit von Vorhersagemodellen wie dem von uns geplanten beeintr√§chtigen kann.
            <br><br>
            Dimensionsprobleme treten h√§ufig auf, wenn mehrere Merkmale stark korrelieren, d. h. sie √ºberlappende oder
            redundante Informationen vermitteln. Diese Redundanz kann die Interpretierbarkeit statistischer Tests f√ºr
            die abh√§ngigen DRI-Produktvariablen beeintr√§chtigen und kann durch Techniken zur <b>Dimensionsreduktion</b>
            behoben werden. Dazu muss eine Korrelationsanalyse durchgef√ºhrt werden.
            Now that the data from all sources are completely aligned it is time to apply data exploration and
            visualization techniques in order to understand the data's characteristics, uncover potential patterns, and
            make informed decisions about which variables are relevant for the modeling phase.
        </h3>

        <section class="sample_code">
            Die <b>Pearson-Korrelationsmatrix</b> misst die lineare Beziehung zwischen zwei kontinuierlichen
            Variablen, was im Fall unseres Datensatzes auf alle vorbereiteten Daten anwendbar ist. Die
            Pearson-Korrelation wird h√§ufig bei der Analyse von Beziehungen zwischen Variablen mit kontinuierlichen
            Daten verwendet, wobei davon ausgegangen wird, dass die Daten normalverteilt sind und dass die Beziehung
            zwischen allen Variablen linear ist.
            <br><br>

            <div id="pearson_heatmap">
                <py-script>
                import pandas as pd
                import seaborn as sns
                import openpyxl
                import matplotlib.pyplot as plt
                from pyscript import display

                df_1 = pd.read_excel('correlation_matrices.xlsx', sheet_name="Pearson Correlation", header=0,
                    index_col=0)

                fig1, ax1 = plt.subplots()
                sns.heatmap(df_1, annot=False, cmap="Blues", ax=ax1, xticklabels=False, yticklabels=False)
                plt.title("Pearson correlation (DRI process)")
                display(fig1, target="pearson_heatmap")
                </py-script>
            </div>

            <br><br>
            Die <b>Spearman-Korrelationsmatrix</b> misst hingegen die St√§rke und Richtung einer monotonen Beziehung
            zwischen zwei Variablen. Der Pearson-Korrelationskoeffizient geht weder von einer Linearit√§t zwischen den
            Variablen noch von einer Normalverteilung der Daten aus, was besonders hilfreich ist, wenn zwischen zwei
            Variablen exponentielle, logarithmische oder sogar polynomiale Beziehungen bestehen.
            <br><br>
            <div id="spearman_heatmap">
                <py-script>
                import pandas as pd
                import seaborn as sns
                import openpyxl
                import matplotlib.pyplot as plt
                from pyscript import display

                df_2 = pd.read_excel('correlation_matrices.xlsx', sheet_name="Spearman Correlation", header=0,
                    index_col=0)

                fig2, ax2 = plt.subplots()
                sns.heatmap(df_2, annot=False, cmap="Greens", ax=ax2, xticklabels=False, yticklabels=False)
                plt.title("Spearman correlation (DRI process)")
                display(fig2, target="spearman_heatmap")
                </py-script>
            </div>
        </section>

        <h3>Techniken zur Dimensionsreduktion sind selbsterkl√§rend, aber sie sind etwas fortgeschrittener als die
            einfache Reduzierung der Anzahl der Variablen, da dabei versehentlich die zugrunde liegende
            Informationsstruktur der Daten verloren gehen k√∂nnte. In diesem Projekt werden zwei sich erg√§nzende
            Techniken zur Dimensionsreduktion angewendet und verglichen: die Hauptkomponentenanalyse (PCA) und die
            umgekehrte schrittweise Regression.
            <br><br>
            Sowohl die <b>umgekehrte schrittweise Regression</b> als auch die <b>Hauptkomponentenanalyse (PCA)</b>
            erfordern eine eingehende Analyse der Beziehungen zwischen unabh√§ngigen Variablen, um festzustellen, welche
            Variablen innerhalb des Datensatzes Redundanzen aufweisen. In diesem Projekt wenden wir nicht nur
            grundlegende Pearson-Korrelationsmetriken an, sondern pr√ºfen auch, ob zwischen den Variablen nicht lineare
            Beziehungen bestehen, indem wir den Spearman-Rangkorrelationskoeffizienten verwenden.
            <br><br>
            Die PCA ist eine statistische Technik, die die urspr√ºnglichen Merkmale in einen neuen Satz unkorrelierter
            Komponenten umwandelt, die nach ihrer F√§higkeit, die Varianz in den Daten zu erkl√§ren, geordnet sind. Sie
            ist besonders n√ºtzlich, wenn versucht wird, Informationen zu erhalten und gleichzeitig die Anzahl der
            Eingabedimensionen f√ºr die Modellierung zu reduzieren. Die kumulative erkl√§rte Varianz wird visualisiert,
            um die optimale Anzahl von Komponenten zu bestimmen, die erforderlich sind, um den gr√∂√üten Teil der
            Informationen zu erhalten. Diese Komponenten werden dann zum Trainieren eines verallgemeinerten additiven
            Modells (GAM) verwendet.
            <br><br>
            Die umgekehrte schrittweise Regression hingegen ist eine Methode zur Merkmalsauswahl, bei der Variablen auf
            der Grundlage ihrer statistischen Signifikanz (p-Werte) iterativ aus einem multivariaten Regressionsmodell
            entfernt werden. Bei diesem Ansatz werden nur diejenigen Pr√§diktoren beibehalten, die einen bedeutenden
            Beitrag zur Zielvariablen leisten, wodurch ein besser interpretierbares Modell in Bezug auf die
            tats√§chlichen Variablennamen anstelle von abstrakten Komponenten entsteht.
            <br><br>
            <b>Hinweis:</b> Die umgekehrte schrittweise Regression und die PCA helfen dabei, zu verstehen, welche
            Variablen den gr√∂√üten Einfluss auf die Abfallerzeugung haben, indem sie die Variablen isolieren, die am
            meisten f√ºr Qualit√§tsschwankungen und Abfallbildung verantwortlich sind <b>(Qualit√§tskontrolle)</b>, und
            erm√∂glichen die Erstellung genauerer Modelle, die das Abfallaufkommen und die Qualit√§tsmerkmale des
            Endprodukts vorhersagen <b>(Qualit√§tsvorhersage)</b>.
        </h3>

        <h3>Das Projekt verwendet zwei verschiedene Techniken zur Dimensionsreduktion, um die Schw√§chen beider
            Verfahren auszugleichen. Bei der ‚ÄûReverse Stepwise Regression‚Äù besteht das Ziel darin, bei jedem Schritt
            den am wenigsten signifikanten Pr√§diktor (basierend auf dem p-Wert) zu entfernen, indem der gesamte
            Datensatz in ein Modell f√ºr die Daten eingepasst wird, dessen Leistung bewertet wird und dann nach und nach
            alle Pr√§diktoren mit der geringsten Signifikanz entfernt werden, bis die Leistung des Modells ihren
            H√∂hepunkt erreicht hat.
            <br><br>
            Die Hauptkomponentenanalyse (PCA) hingegen basiert auf der Umwandlung des Datensatzes in Hauptkomponenten
            durch die Verwendung der <b>Eigenzerlegung</b>, die die Gruppe der abh√§ngigen Variablen in ihre jeweiligen
            Eigenvektoren und Eigenwerte zerlegt. Die PCA w√§hlt dann die wichtigsten Hauptkomponenten aus, indem sie
            Eigenvektoren auf der Grundlage ihrer entsprechenden Eigenwerte ausw√§hlt.
            <br><br>
            Leider geht die PCA auf Kosten der <b>Interpretierbarkeit</b>, da diese Komponenten lineare Kombinationen
            der urspr√ºnglichen Merkmale sind. Die umgekehrte schrittweise Regression hingegen beh√§lt die tats√§chlichen
            Merkmalsnamen bei und bietet Transparenz dar√ºber, welche Variablen f√ºr die Abfallerzeugung und
            Produktqualit√§t am wichtigsten sind. Das bedeutet, dass sowohl die Ergebnisse der PCA als auch die
            Ergebnisse der umgekehrten schrittweisen Regression f√ºr das Training des Prognosemodells verwendet werden
            k√∂nnten.

        </h3>

        <h3><b>SCHRITT 5: Entwicklung von Merkmalen</b><br><br>
            In beiden F√§llen m√ºssen wir den Datensatz vorverarbeiten, indem wir alle Daten gleich skalieren. Dies
            geschieht, weil die meisten unabh√§ngigen Variablen nicht dieselben Metriken haben (Dr√ºcke, Temperaturen,
            Prozents√§tze, Messungen usw.). Es gibt viele verschiedene Skalierungsmethoden aus der Bibliothek
            sklearn.preprocessing, wie StandardScaler, MinMaxScaler, RobustScaler usw., die alle das gleiche Ziel
            haben, n√§mlich die Standardisierung der Daten.
            <br><br>
            Die Standardisierung stellt sicher, dass alle Variablen gleicherma√üen zum Modelltraining und zur
            Komponentengenerierung beitragen. In diesem Projekt wird MinMaxScaler ausgew√§hlt, nachdem sowohl der
            Shapiro-Wilk-Test als auch der Kolmogorov-Smirnov-Test auf Normativit√§t angewendet wurden und best√§tigt
            wurde, dass die meisten Variablen KEINER Normalverteilung folgen. Dies eignet sich besonders gut f√ºr die
            Vorbereitung industrieller Prozessdaten f√ºr die nachgelagerte Modellierung, da MinMaxScaler eine lineare
            Skalierung auf einen festen Bereich (-1 bis 1) verwendet und dabei die urspr√ºngliche Form aller
            nicht-normalen Verteilungen beibeh√§lt.
            <br><br>
            Nach der Skalierung werden die Daten durch ein PCA-Transformer-Objekt geleitet, um die Mindestanzahl an
            Komponenten zu ermitteln, die erforderlich ist, um den gr√∂√üten Teil der Varianz des Datensatzes
            beizubehalten und gleichzeitig die Mindestanzahl an Variablen f√ºr die Regression beizubehalten.
        </h3>

        <section class="sample_code">
            <b># Python CODE: MinMaxScaling<br>
            Nach Anwendung der beiden Tests ‚ÄûShapiro-Wilk‚Äù und ‚ÄûKolmogorov-Smirnov‚Äù zur √úberpr√ºfung der
            Normalverteilung und der Anpassungsg√ºte. k√∂nnen wir feststellen, dass keine der unabh√§ngigen Variablen
            einer Normalverteilung folgt, was uns dazu veranlasst, den MinMaxScaler zu verwenden, da dieser besonders
            effektiv ist, wenn die Daten nicht normalverteilt sind. Dabei wird eine Skalierungsmethode angewendet, die
            keine Verteilung voraussetzt und die Daten auf einen Bereich von [-1, 1] skaliert, wobei die urspr√ºngliche
            nicht normale Verteilung beibehalten wird.<br><br>
            Hinweis: MinMaxScaler wendet eine einheitliche Skalierung an, indem er den Minimalwert subtrahiert und
            durch den Bereich des Datensatzes dividiert.</b><br>

            <br>from sklearn.preprocessing import MinMaxScaler
            <br>scaler = MinMaxScaler()
            <br>independent_variables = final_variables.iloc[:, 1:].copy()
            <br>independent_variables.columns = independent_variables.columns.astype(str)
            <br>independent_variables = scaler.fit_transform(independent_variables)<br><br>

            <b># Python CODE: Principal Component Analysis (PCA)<br>
            In scikit-learn wird PCA als Transformer-Objekt implementiert, das Komponenten in seiner fit()-Funktion
            lernt, die die Kovarianzmatrix berechnet, die Eigenzerlegung der Kovarianzmatrix durchf√ºhrt und
            anschlie√üend die Hauptkomponenten generiert.</b><br>
            <br>from sklearn.decomposition import PCA
            <br>pca_dri = PCA()
            <br>pca_dri.fit(independent_variables)
            <br>principal_components = pca_dri.transform(independent_variables)
            <br>selected_components = principal_components[:, :6]
            <br><br><b>Die PCA-Komponenten werden sp√§ter in ein Regressionsmodell des Typs General Additive Model (GAM)
            eingespeist, das die Komplexit√§t der Beziehungen zwischen den unabh√§ngigen und abh√§ngigen Variablen
            ber√ºcksichtigt und eine eingehende statistische Analyse des gesamten Prozesses bietet.</b>
        </section>

        <h3>Dank der Funktion ‚Äûexplained_variance_ratio_‚Äú in sklearn.decomposition k√∂nnen wir den Prozentsatz der
            Varianz messen, der durch jede der generierten Hauptkomponenten erkl√§rt wird. Durch Berechnung der
            kumulierten Summe der Verh√§ltnisse k√∂nnen wir feststellen, dass die Gesamtvarianz der ersten 6
            PCA-Komponenten eine Varianz von 92,39 % erreicht. (Bei 5 Komponenten betr√§gt der Prozentsatz 87,97 % und
            bei 7 PCA-Komponenten erreicht die Varianz 93,61 %.)
            <br><br>
            <b>Hinweis:</b> Bei Verwendung von 14 oder mehr Komponenten betr√§gt die explained_variance_ratio_ 1,0,
            was bedeutet, dass wir 100 % der Varianz im Datensatz erfasst haben.
            <br><br>
            Die Merkmalsauswahl aus der umgekehrten schrittweisen Regression kann als n√ºtzliche √úberpr√ºfung der
            Ergebnisse aus der PCA dienen. W√§hrend die PCA Muster allein auf der Grundlage der Varianz identifiziert,
            w√§hlt die schrittweise Regression Merkmale auf der Grundlage ihrer Vorhersagekraft aus. Wenn dieselben
            Variablen in beiden Methoden als wichtig erscheinen, erh√∂ht dies die Zuversicht, dass diese Merkmale
            tats√§chlich Einfluss auf den Prozess haben und nicht nur statistisches Rauschen sind.
            <br><br>
            Durch die kombinierte Verwendung von PCA und Reverse Stepwise Regression k√∂nnen wir die Dimensionalit√§t
            reduzieren und gleichzeitig die Interpretierbarkeit des Modells erhalten. PCA hilft dabei, den Datensatz zu
            vereinfachen und Multikollinearit√§t zu bew√§ltigen, w√§hrend die schrittweise Regression die spezifischen
            Variablen hervorhebt, die f√ºr die Vorhersage der Ergebnisse am wichtigsten sind. Dieser kombinierte Ansatz
            st√§rkt die Zuverl√§ssigkeit des Modells und stellt sicher, dass es den zugrunde liegenden Produktionsprozess
            widerspiegelt.
        </h3>

        <h3>Ohne Skalierung w√ºrde die PCA irref√ºhrende Hauptkomponenten liefern und die Interpretierbarkeit und
            Leistung Ihres nachfolgenden GAM-Modells beeintr√§chtigen. Wenn Variablen sehr unterschiedliche Bereiche
            haben, k√∂nnen kleine Fehler in Variablen mit gr√∂√üeren Werten das Ergebnis unverh√§ltnism√§√üig stark
            beeinflussen. Die Skalierung mildert dies, indem sie alle Variablen auf eine vergleichbare Basis stellt.
            Dies ist besonders wichtig in multikollinearen Systemen ‚Äì wie Ihrem ‚Äì, in denen geringf√ºgige
            Ungleichgewichte in der Skalierung Beziehungen verschleiern oder √ºbertreiben k√∂nnen.
        </h3>

        <video id="video_1" controls autoplay loop muted>
            <source src="videos/project_drei.mp4" type="video/mp4">
        </video>

        <h3><b>SCHRITT 6: Aufbau von Modellen</b><br><br>
            Wie bereits erw√§hnt, wird das Generalized Additive Model (GAM) verwendet, um die Beziehung zwischen den
            Ausgangsvariablen  und den unabh√§ngigen Variablen, also den Rohstoffen und dem DRI-Prozess, zu sch√§tzen.
            GAM dient als Erweiterung linearer Modelle, die nichtlineare Effekte durch die Anwendung von
            Gl√§ttungsfunktionen auf jeden der PCA-Pr√§diktoren ber√ºcksichtigen.
            <br><br>
            Jede PCA-Komponente wird in GAM eingef√ºhrt und dann mithilfe von ‚ÄûSplines‚Äù durch die Gl√§ttungsfunktion
            transformiert. Splines sind glatte, flexible mathematische Funktionen, die zur Modellierung nichtlinearer
            Beziehungen zwischen Variablen verwendet werden.
            <br><br>
            Da die PCA-Komponenten Eigenwerte enthalten, die die Relevanz f√ºr die Ausgangsvariablen messen, indem sie
            die Varianz jeder Komponente erfassen. Es werden nur Komponenten beibehalten, die nicht miteinander
            korrelieren (orthogonal) und gro√üe Eigenwerte aufweisen (mehr Informationen enthalten).
        </h3>

        <h3>Nachdem die Hauptkomponenten (PC) durch die Splei√üfunktionen transformiert wurden, nimmt die GAM-Struktur
            die folgende Form an:<br>
            <code>s(0) + s(1) + ... + s(5)</code>, wobei <code>s(i)</code> eine Spline-Funktion ist, die auf die i-te
            Hauptkomponente angewendet wird.
            <br><br>
            Die 9 abh√§ngigen (Ausgangs-)Variablen verwenden denselben Satz von PCA-abgeleiteten Merkmalen, um die
            Interpretierbarkeit der Daten zu verbessern, indem sie die Prognosewerte des Modells analysieren und eine
            gezielte Bewertung erm√∂glichen, wie jede Ausgangsvariable durch die latente Struktur der Rohstoff- und
            Prozessdaten beeinflusst wird.
            <br><br>
            Um die Leistung jedes Modells zu bewerten, werden Streudiagramme verwendet, um die vorhergesagten Werte mit
            den tats√§chlich beobachteten Werten zu vergleichen.
        </h3>

        <img src="bilder/project_drei4.png" alt="Project Scatter Graph 1">

        <h3>Diese Diagramme enthalten eine glatte Vorhersagekurve, die √ºber die Streuung der tats√§chlichen Werte gelegt
            wird, wodurch sowohl die Modellanpassung als auch die Restvarianz visualisiert werden k√∂nnen. Die erste
            Hauptkomponente ist f√ºr die Visualisierung besonders n√ºtzlich, da sie in der Regel die gr√∂√üte Varianzquelle
            in den Eingabedaten erfasst und somit ein aussagekr√§ftiger Indikator f√ºr das vorherrschende
            Prozessverhalten ist.
            <br><br>
            Nach der Erstellung des GAM-Modells unter Verwendung von PCA-transformierten Eingaben besteht der n√§chste
            Schritt darin, den Modellierungsprozess unter Verwendung der umgekehrten schrittweisen Regression zu
            wiederholen. Diese Technik hilft dabei, die relevantesten urspr√ºnglichen Merkmale anstelle der
            transformierten Merkmale zu identifizieren, wodurch die Erkenntnisse des Modells direkt mit den
            physikalischen Variablen aus den Rohstoff- oder DRI-Prozessdaten verkn√ºpft werden.
        </h3>

        <h3>Unter Verwendung der statsmodels-Bibliothek beginnt die inverse schrittweise Regression mit allen im Modell
            enthaltenen Variablen. Die Variable mit dem h√∂chsten p-Wert (was auf die geringste statistische Signifikanz
            hinweist) wird entfernt, und das Modell wird neu angepasst. Diese iterative Eliminierung wird fortgesetzt,
            bis nur noch die 6 statistisch signifikantesten Variablen √ºbrig bleiben. Diese ausgew√§hlten Merkmale werden
            dann als Eingaben f√ºr ein zweites GAM-Modell verwendet, das derselben splinebasierten Struktur folgt wie
            das aus der PCA abgeleitete Modell.
            <br><br>
            Durch den Vergleich der Leistung und Interpretierbarkeit beider Modelle ‚Äì eines basierend auf
            PCA-Komponenten, das andere auf statistisch ausgew√§hlten Rohmerkmalen ‚Äì k√∂nnen wir die Kompromisse zwischen
            Dimensionsreduktion und Variablentransparenz bewerten. Das PCA-basierte Modell bietet m√∂glicherweise eine
            gleichm√§√üigere Leistung und weniger Rauschen, w√§hrend das schrittweise Modell klarere Verbindungen zu
            bestimmten Prozessparametern herstellt, was f√ºr technische Erkenntnisse und Entscheidungsfindungen wertvoll
            ist.
        </h3>

        <img src="bilder/project_drei5.png" alt="Project Scatter Graph 2">

        <h3>Diese Logik wird besser in den beiden Grafiken dargestellt, die als Visualisierung der Modelldiagnose aus
            den Vorhersagen des Generalized Additive Model (GAM) dienen. Beide Grafiken stellen die Beziehung zwischen
            zwei verschiedenen Zielvariablen mit derselben Hauptkomponente dar. (Dies ist die erste Hauptkomponente
            innerhalb des GAM)
            <br><br>
            Die erste Grafik zeigt die Beziehung zwischen der erzeugten Schlacke (eine Art Abfall aus dem DR-Prozess)
            und der ersten Hauptkomponente. Die schwarze Linie stellt die Prognosewerte des Modells dar, die eine
            starke negative Korrelation aufweisen: Mit zunehmender erster Hauptkomponente nimmt die Schlacke
            tendenziell ab.
            <br><br>
            Die zweite Grafik hingegen zeigt die Beziehung zwischen dem DRI-Pellet-Anteil (der die Gesamtqualit√§t des
            endg√ºltigen Pelletprodukts darstellt) und der ersten Hauptkomponente. Im Gegensatz zur ersten Grafik zeigt
            die zweite Grafik jedoch, dass zwischen beiden Elementen keine starke Beziehung besteht.
        </h3>

        <h3><b>SCHRITT 7: Modelltraining und -bewertung</b><br><br>
            Um die Vorhersagegenauigkeit und Verallgemeinerbarkeit beider Modellierungsans√§tze ‚Äì PCA-basierte GAM und
            Reverse Stepwise Regression-basierte GAM ‚Äì zu bewerten, wurde eine Reihe von Diagnosemetriken verwendet:
            Generalized Cross-Validation (GCV), Akaike Information Criterion (AIC) und Pseudo R-squared. Diese Metriken
            erm√∂glichen die Bewertung, wie gut jedes Modell zu den Daten passt, aber auch, wie gut es bei unbekannten
            Eingaben voraussichtlich abschneiden wird.
            <br><br>
            Der GCV-Wert ist eine Methode zur Sch√§tzung des Vorhersagefehlers eines Modells, insbesondere in
            Situationen, in denen Regularisierungstechniken wie Ridge-Regression zum Einsatz kommen. Es handelt sich
            dabei um eine bestrafte Fehlermetrik, die die Modellkomplexit√§t zwischen zwei verschiedenen Modellen
            ber√ºcksichtigt. GCV bietet eine M√∂glichkeit, die Vorhersageleistung eines Modells zu bewerten und seine
            Parameter zu optimieren.
        </h3>

        <h3>Das PCA-basierte GAM erreichte einen GCV von 0,0001, was auf einen minimalen Vorhersagefehler und eine
            reibungslose Verallgemeinerung auf neue Daten hindeutet. Das Reverse Stepwise Regression-Modell hingegen
            lieferte einen GCV von 0,0, was zwar optimal erscheint, aber tats√§chlich eine √úberanpassung widerspiegeln
            k√∂nnte, bei der das Modell zu perfekt zu den Trainingsdaten passt und m√∂glicherweise nicht gut
            verallgemeinert werden kann.
            <br><br>
            Das bedeutet, dass das Modell, um mit neuen Daten zu funktionieren, neue Trainings-Test-Aufteilungen oder
            Kreuzvalidierungen ben√∂tigt, um sicherzustellen, dass das Modell gut generalisiert und nicht √ºberm√§√üig an
            bestimmte Muster im Trainingssatz angepasst ist.
            <br><br>
            Das Akaike-Informationskriterium (AIC) ist ein statistisches Ma√ü, das zur Bewertung der relativen Qualit√§t
            der beiden verschiedenen GAMs verwendet wird. Es w√§gt die Bedeutung zwischen Modellanpassung
            (Wahrscheinlichkeit) und Modellkomplexit√§t (Anzahl der Parameter) ab, um √úberanpassung und schlechte
            Leistung bei neuen Daten zu vermeiden. Niedrigere AIC-Werte weisen im Allgemeinen auf ein besser
            angepasstes und sparsameres Modell hin.
        </h3>

        <h3>√Ñhnlich wie der GCV-Wert bestraft das AIC Modelle mit mehr Parametern und verhindert so √ºberm√§√üig komplexe
            Modelle, die zu einer √úberanpassung der Daten f√ºhren k√∂nnten. Die Messung des AIC in beiden Modellen zeigt,
            dass das PCA-Modell ein deutlich niedrigeres AIC aufweist. Dies st√ºtzt die Annahme, dass das PCA-basierte
            Modell trotz der abstrakten Erstellung von Hauptkomponenten eine starke Vorhersagekraft erreicht und
            gleichzeitig unn√∂tige Komplexit√§t vermeidet.
            <br><br>
            In diesem Projekt ist eine der wichtigsten Kennzahlen zur Bewertung der Modellleistung das R-Quadrat (R¬≤),
            auch bekannt als Bestimmtheitsma√ü. R¬≤ misst, wie gut ein Modell die Variabilit√§t der Zielvariablen auf der
            Grundlage der Eingaben erkl√§rt. Ein R¬≤ von 0,82 bedeutet beispielsweise, dass 82 % der Variation einer
            DRI-Qualit√§tskennzahl (wie Schlackengehalt oder Pelletgr√∂√üe) durch die Pr√§diktoren des Modells erkl√§rt
            werden.
            <br><br>
            Bei der Anwendung auf verallgemeinerte additive Modelle (GAMs) bezeichnen wir diese Metrik als
            Pseudo-R-Quadrat, da GAMs gl√§ttende Splines und eine bestrafte Wahrscheinlichkeitssch√§tzung verwenden, die
            sich von der traditionellen linearen Regression unterscheiden. In diesem Zusammenhang wird Pseudo-R¬≤ als
            Anteil der durch das Modell reduzierten Abweichung berechnet und bietet eine √§hnliche Interpretation: Je
            h√∂her der Wert, desto besser passt das Modell zu den Daten.
        </h3>

        <h3>Das PCA-basierte GAM-Modell erreichte einen Pseudo-R¬≤-Wert von 0,3382, was bedeutet, dass etwa 33,82 % der
            Variabilit√§t im Ergebnis allein mit den sechs wichtigsten Hauptkomponenten erkl√§rt werden konnten. Dies mag
            zwar moderat erscheinen, spiegelt jedoch ein Modell wider, das Generalisierung und Robustheit in Einklang
            bringt. Im Gegensatz dazu erreichte das Reverse Stepwise Regression-Modell einen viel h√∂heren
            Pseudo-R¬≤-Wert von 0,8252, was eine starke Erkl√§rungskraft zeigt, aber auch auf ein h√∂heres Risiko der
            √úberanpassung hindeutet ‚Äì insbesondere in Verbindung mit einem ungew√∂hnlich niedrigen GCV-Wert.
            <br><br>
            Diese Abweichung wirft einen wichtigen Kompromiss auf. Das Reverse-Stepwise-Modell weist zwar eine st√§rkere
            Erkl√§rungskraft auf, jedoch m√∂glicherweise auf Kosten der Verallgemeinerbarkeit, wie der GCV nahelegt. Das
            PCA-Modell scheint durch die Reduzierung von Rauschen und Redundanz mittels Dimensionsreduktion ein
            besseres Gleichgewicht zwischen Vorhersageleistung und Modellstabilit√§t zu erzielen.
        </h3>

        <h3><b>SCHRITT 8: Modellverbesserung</b><br><br>
            Durch die Bewertung beider GAMs. Einer verwendete PCA-Komponenten. Der andere verwendete statistisch
            ausgew√§hlte Merkmale aus der Reverse Stepwise Regression. Es wurde deutlich, dass beide Ans√§tze als
            Werkzeuge zur Dimensionsreduktion dienen. Sie unterscheiden sich jedoch erheblich in Bezug auf
            Informationserhalt, Modellkomplexit√§t und Interpretierbarkeit.
            <br><br>
            Eine wesentliche St√§rke des PCA-basierten Modells ist seine F√§higkeit, nahezu die gesamte urspr√ºngliche
            Varianz zu erhalten (z. B. werden 92,39 % durch sechs Komponenten erfasst), ohne zu √ºberanpassen. Dies
            erm√∂glicht dem generalisierten additiven Modell (GAM) eine gute Verallgemeinerung, wodurch Rauschen und
            Multikollinearit√§t reduziert werden, w√§hrend die Vorhersagekraft erhalten bleibt. Dies ist besonders
            wertvoll bei der Verarbeitung hochdimensionaler Daten, wie sie in industriellen Sensornetzwerken und bei
            der √úberwachung chemischer Prozesse √ºblich sind.
        </h3>

        <h3>Allerdings transformiert die PCA die Variablen in latente Komponenten, was eine direkte Interpretation
            erschweren kann. Im Gegensatz zu Rohdaten wie Reagenzfluss oder Druckwerten entsprechen Hauptkomponenten
            nicht einzelnen physikalischen Gr√∂√üen. Diese mangelnde Interpretierbarkeit kann ihre N√ºtzlichkeit bei der
            Prozessdiagnostik und bei Korrekturma√ünahmen einschr√§nken, bei denen Fachleute oft die genaue Variable
            kennen m√ºssen, die angepasst werden muss.
            <br><br>
            Neben dem Vergleich der Methoden sollte bei der Modellverfeinerung auch die Skalierbarkeit und das
            Einsatzpotenzial ber√ºcksichtigt werden. In Echtzeit-Fertigungssystemen m√ºssen Modelle effizient und robust
            genug sein, um Sensorrauschen zu widerstehen und sich an Prozessabweichungen anzupassen. In diesem
            Zusammenhang bieten PCA-basierte Modelle eine g√ºnstige Balance, indem sie den Merkmalsraum vereinfachen,
            die Laufzeitleistung verbessern und Widerstandsf√§higkeit gegen√ºber Schwankungen einzelner Sensorwerte
            bieten.
            <br><br>
            Es k√∂nnen mehrere Verfeinerungsstrategien eingef√ºhrt werden, um die Modellleistung, Zuverl√§ssigkeit und
            Benutzerfreundlichkeit in realen Fertigungsumgebungen weiter zu verbessern. Beispielsweise verwendet das
            aktuelle Modell Standard-Lambda-Werte. Die manuelle oder automatisierte Kreuzvalidierung zur Optimierung
            von Lambda f√ºr jede Variable k√∂nnte das endg√ºltige Modell verbessern. Dieser Prozess passt die Flexibilit√§t
            jeder Spline an, um das optimale Gleichgewicht zwischen der Erfassung wichtiger Trends und der Vermeidung
            von Rauschen zu finden.
        </h3>

        <h3>Ein weiterer wichtiger Bereich f√ºr zuk√ºnftige Verbesserungen ist die Integration von
            Residualanalyseverfahren in den Workflow zur Modellbewertung und -verfeinerung. Durch die systematische
            Analyse von Residualmustern l√§sst sich feststellen, ob bestimmte Zusammenh√§nge √ºbersehen wurden, ob
            nichtlineare Effekte unzureichend erfasst wurden oder ob das Modell unter bestimmten Prozessbedingungen
            durchweg zu hohe oder zu niedrige Vorhersagen liefert.
            <br><br>
            Die Einbeziehung dieser Techniken verbessert nicht nur die Vorhersagegenauigkeit, sondern st√§rkt auch die
            Zuverl√§ssigkeit und Transparenz des Modells, wodurch es einfacher in realen Produktionsumgebungen
            eingesetzt werden kann. In industriellen Anwendungen, in denen Entscheidungen auf der Grundlage von
            Modellergebnissen Auswirkungen auf Kosten, Sicherheit und Produktqualit√§t haben k√∂nnen, sind Konsistenz und
            Interpretierbarkeit ebenso wichtig wie die Leistung.
            <br><br>
            Aus diesem Grund ist die Modellverfeinerung ein wesentlicher Schritt in jedem Projekt im Bereich
            maschinelles Lernen oder statistische Modellierung. Sie bietet eine strukturierte M√∂glichkeit, die
            Ergebnisse durch technische Anpassungen ‚Äì wie Hyperparameter-Tuning, diagnostische Analyse oder
            Regularisierung ‚Äì zu verbessern, ohne die Gesamtstruktur oder das Ziel des Modells zu verwerfen. Diese
            Verfeinerungen k√∂nnen oft zu erheblichen Verbesserungen der Leistung und Benutzerfreundlichkeit f√ºhren,
            wodurch das Modell widerstandsf√§higer, effizienter und besser auf die betrieblichen Anforderungen
            abgestimmt wird. Im Fertigungskontext f√ºhrt dies direkt zu einer besseren Prozesskontrolle, weniger
            Ausschuss und einer gleichm√§√üigeren Produktqualit√§t.
        </h3>

        <h3><b>FAZIT:</b><br>
            Dieses Projekt zeigt das Potenzial der Kombination von <b>verallgemeinerten additiven Modellen</b> mit
            Techniken zur Dimensionsreduktion wie der <b>Hauptkomponentenanalyse</b> und der
            <b>umgekehrten schrittweisen Regression</b> zur Verbesserung der Qualit√§tskontrolle und Abfallprognose im
            Zusammenhang mit der <b>Produktion von direkt reduziertem Eisen (DRI)</b>.
            <br><br>
            Das GAM-Framework zeichnet sich durch seine F√§higkeit aus, nichtlineare Beziehungen zwischen Eingaben und
            Ausgaben durch glatte Spline-Funktionen zu erfassen. Dies ist besonders wichtig in industriellen
            Umgebungen, in denen viele Variablen auf nicht offensichtliche Weise miteinander interagieren. Die PCA
            unterst√ºtzt diesen Modellierungsansatz, indem sie einen hochdimensionalen Merkmalsraum in einen kompakten
            und rauschresistenten Satz von Komponenten umwandelt, was zur Verbesserung der Generalisierung und zur
            Verringerung der Rechenkomplexit√§t beitr√§gt. Gleichzeitig stellt die Reverse Stepwise Regression sicher,
            dass die Interpretierbarkeit erhalten bleibt, indem sie die statistisch signifikantesten
            Rohprozessvariablen identifiziert und so direkte Verbindungen zwischen dem Modellverhalten und dem
            technischen Wissen herstellt.
        </h3>

        <h3>Diese Modelle erm√∂glichen nicht nur eine genaue Vorhersage wichtiger Qualit√§tsindikatoren (z. B.
            Schlackengehalt, Porosit√§t, Metallisierung), sondern unterst√ºtzen auch die Ursachenanalyse, indem sie
            aufzeigen, welche Prozessbedingungen die Qualit√§t des Endprodukts am st√§rksten beeinflussen. Die
            Untersuchung von <b>glatten Spline-Funktionen</b> und <b>Koeffizienten√ºbersichten</b> erm√∂glicht es
            Ingenieuren, einflussreiche Parameter zu identifizieren und deren Wechselwirkungen zu verstehen ‚Äì und
            liefert so umsetzbare Erkenntnisse zur Verbesserung von Effizienz und Nachhaltigkeit.
            <br><br>
            Dar√ºber hinaus schafft dieses Projekt eine Blaupause f√ºr datengest√ºtzte Entscheidungen in
            Fertigungsumgebungen. Es zeigt, wie pr√§diktive Modellierung √ºber die √úberwachung hinausgehen kann, um
            Fr√ºhwarnungen zu unterst√ºtzen, die Ressourcennutzung zu optimieren und das Abfallaufkommen zu reduzieren.
            Die Interpretierbarkeit und Flexibilit√§t von GAMs machen sie besonders geeignet f√ºr den Einsatz in
            Echtzeit-Steuerungssystemen, wo sowohl Transparenz als auch Anpassungsf√§higkeit erforderlich sind.
            <br><br>
            Zusammenfassend l√§sst sich sagen, dass die Integration datengest√ºtzter Modelle wie GAM in industrielle
            Systeme die traditionelle Prozess√ºberwachung in einen proaktiven und pr√§diktiven Rahmen verwandeln kann,
            der Abfall reduziert, die Ausgabequalit√§t verbessert und datengest√ºtzte Entscheidungen in gro√üem Ma√üstab
            unterst√ºtzt. Dieser Rahmen ist besonders leistungsf√§hig in datenreichen Umgebungen, in denen komplexe
            Prozessdynamiken nicht mehr allein durch manuelle Steuerung und statische Regeln vollst√§ndig verwaltet
            werden k√∂nnen.
        </h3>
    </div>
</section>

  <footer>
    <p>Erstellt von Adri√°n Ochoa Ferri√±o - 2023</p>
  </footer>

</body>
</html>